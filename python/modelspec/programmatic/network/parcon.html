<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ParallelContext &mdash; NEURON  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=87e54e7c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=6933245a" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="&lt;no title&gt;" href="lyttonmpi.html" />
    <link rel="prev" title="Networks" href="../network.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            NEURON
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Building:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cmake_doc/index.html">CMake Build Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/developer.html">Developer Builds</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../videos/index.html">Training videos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide/index.html">Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../courses/exercises2018.html">NEURON Course Exercises</a></li>
<li class="toctree-l1"><a class="reference external" href="https://neuron.yale.edu/phpBB">The NEURON forum</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../publications.html">Publications about NEURON</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../publications-using-neuron.html">Publications using NEURON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NEURON scripting:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../scripting.html">Running Python and HOC scripts</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">NEURON Python documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#quick-links">Quick Links</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#basic-programming">Basic Programming</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#model-specification">Model Specification</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../guitools.html">Model Specification GUI Tools</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../../programmatic.html">Programmatic Model Specification</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../topology.html">Topology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ions.html">Ions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mechanisms.html">Dynamics (Channels, etc…)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rxd.html">Basic Reaction-Diffusion</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../network.html">Networks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../electrod.html">Electrode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mechtype.html">MechanismType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../ste.html">StateTransitionEvent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../obsoletestimuli.html">Obsolete Stimuli</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#simulation-control">Simulation Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#visualization">Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#analysis">Analysis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hoc/index.html">NEURON HOC documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../otherscripting.html">Other scripting languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Python tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rxd-tutorials/index.html">Python RXD tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../coreneuron/index.html">CoreNEURON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NMODLanguage:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../nmodl/index.html">NMODLanguage</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../scm/index.html">NEURON SCM and Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dev/index.html">NEURON Development topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../doxygen.html">C/C++ API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Removed Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../removed_features.html">Removed Features</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html">NEURON 8.2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html#neuron-8-1">NEURON 8.1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html#neuron-8-0">NEURON 8.0</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html#contributors">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html#feedback-help">Feedback / Help</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">NEURON</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">NEURON Python documentation</a></li>
          <li class="breadcrumb-item"><a href="../../programmatic.html">Programmatic Model Specification</a></li>
          <li class="breadcrumb-item"><a href="../network.html">Networks</a></li>
      <li class="breadcrumb-item active">ParallelContext</li>
<li class="wy-breadcrumbs-aside">
    
    
        
        <a href="../../../../..html/../hoc/modelspec/programmatic/network/parcon.html" >  Switch to HOC</a>
    
</li>

      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/python/modelspec/programmatic/network/parcon.rst.txt" rel="nofollow"> View page source</a>
      </li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><dl class="docutils"><dt><a href="#ParallelContext" title="Link to this definition">ParallelContext</a></dt><dd><a href="#ParallelContext.allgather" title="Link to this definition">allgather</a> &middot; <a href="#ParallelContext.allreduce" title="Link to this definition">allreduce</a> &middot; <a href="#ParallelContext.alltoall" title="Link to this definition">alltoall</a> &middot; <a href="#ParallelContext.barrier" title="Link to this definition">barrier</a> &middot; <a href="#ParallelContext.broadcast" title="Link to this definition">broadcast</a> &middot; <a href="#ParallelContext.cell" title="Link to this definition">cell</a> &middot; <a href="#ParallelContext.context" title="Link to this definition">context</a> &middot; <a href="#ParallelContext.done" title="Link to this definition">done</a> &middot; <a href="#ParallelContext.dt" title="Link to this definition">dt</a> &middot; <a href="#ParallelContext.event_time" title="Link to this definition">event_time</a> &middot; <a href="#ParallelContext.get_partition" title="Link to this definition">get_partition</a> &middot; <a href="#ParallelContext.gid2cell" title="Link to this definition">gid2cell</a> &middot; <a href="#ParallelContext.gid2obj" title="Link to this definition">gid2obj</a> &middot; <a href="#ParallelContext.gid_clear" title="Link to this definition">gid_clear</a> &middot; <a href="#ParallelContext.gid_connect" title="Link to this definition">gid_connect</a> &middot; <a href="#ParallelContext.gid_exists" title="Link to this definition">gid_exists</a> &middot; <a href="#ParallelContext.id" title="Link to this definition">id</a> &middot; <a href="#ParallelContext.id_bbs" title="Link to this definition">id_bbs</a> &middot; <a href="#ParallelContext.id_world" title="Link to this definition">id_world</a> &middot; <a href="#ParallelContext.integ_time" title="Link to this definition">integ_time</a> &middot; <a href="#ParallelContext.look" title="Link to this definition">look</a> &middot; <a href="#ParallelContext.look_take" title="Link to this definition">look_take</a> &middot; <a href="#ParallelContext.max_histogram" title="Link to this definition">max_histogram</a> &middot; <a href="#ParallelContext.mech_time" title="Link to this definition">mech_time</a> &middot; <a href="#ParallelContext.mpi_init" title="Link to this definition">mpi_init</a> &middot; <a href="#ParallelContext.mpiabort_on_error" title="Link to this definition">mpiabort_on_error</a> &middot; <a href="#ParallelContext.multisplit" title="Link to this definition">multisplit</a> &middot; <a href="#ParallelContext.nhost" title="Link to this definition">nhost</a> &middot; <a href="#ParallelContext.nhost_bbs" title="Link to this definition">nhost_bbs</a> &middot; <a href="#ParallelContext.nhost_world" title="Link to this definition">nhost_world</a> &middot; <a href="#ParallelContext.nrncore_run" title="Link to this definition">nrncore_run</a> &middot; <a href="#ParallelContext.nrncore_write" title="Link to this definition">nrncore_write</a> &middot; <a href="#ParallelContext.nthread" title="Link to this definition">nthread</a> &middot; <a href="#ParallelContext.nworker" title="Link to this definition">nworker</a> &middot; <a href="#ParallelContext.optimize_node_order" title="Link to this definition">optimize_node_order</a> &middot; <a href="#ParallelContext.outputcell" title="Link to this definition">outputcell</a> &middot; <a href="#ParallelContext.pack" title="Link to this definition">pack</a> &middot; <a href="#ParallelContext.partition" title="Link to this definition">partition</a> &middot; <a href="#ParallelContext.post" title="Link to this definition">post</a> &middot; <a href="#ParallelContext.prcellstate" title="Link to this definition">prcellstate</a> &middot; <a href="#ParallelContext.psolve" title="Link to this definition">psolve</a> &middot; <a href="#ParallelContext.py_allgather" title="Link to this definition">py_allgather</a> &middot; <a href="#ParallelContext.py_alltoall" title="Link to this definition">py_alltoall</a> &middot; <a href="#ParallelContext.py_broadcast" title="Link to this definition">py_broadcast</a> &middot; <a href="#ParallelContext.py_gather" title="Link to this definition">py_gather</a> &middot; <a href="#ParallelContext.py_scatter" title="Link to this definition">py_scatter</a> &middot; <a href="#ParallelContext.pyret" title="Link to this definition">pyret</a> &middot; <a href="#ParallelContext.retval" title="Link to this definition">retval</a> &middot; <a href="#ParallelContext.runworker" title="Link to this definition">runworker</a> &middot; <a href="#ParallelContext.sec_in_thread" title="Link to this definition">sec_in_thread</a> &middot; <a href="#ParallelContext.send_time" title="Link to this definition">send_time</a> &middot; <a href="#ParallelContext.set_gid2node" title="Link to this definition">set_gid2node</a> &middot; <a href="#ParallelContext.set_maxstep" title="Link to this definition">set_maxstep</a> &middot; <a href="#ParallelContext.setup_transfer" title="Link to this definition">setup_transfer</a> &middot; <a href="#ParallelContext.source_var" title="Link to this definition">source_var</a> &middot; <a href="#ParallelContext.spike_compress" title="Link to this definition">spike_compress</a> &middot; <a href="#ParallelContext.spike_record" title="Link to this definition">spike_record</a> &middot; <a href="#ParallelContext.spike_statistics" title="Link to this definition">spike_statistics</a> &middot; <a href="#ParallelContext.splitcell" title="Link to this definition">splitcell</a> &middot; <a href="#ParallelContext.step_time" title="Link to this definition">step_time</a> &middot; <a href="#ParallelContext.submit" title="Link to this definition">submit</a> &middot; <a href="#ParallelContext.subworlds" title="Link to this definition">subworlds</a> &middot; <a href="#ParallelContext.t" title="Link to this definition">t</a> &middot; <a href="#ParallelContext.take" title="Link to this definition">take</a> &middot; <a href="#ParallelContext.target_var" title="Link to this definition">target_var</a> &middot; <a href="#ParallelContext.thread_busywait" title="Link to this definition">thread_busywait</a> &middot; <a href="#ParallelContext.thread_ctime" title="Link to this definition">thread_ctime</a> &middot; <a href="#ParallelContext.thread_how_many_proc" title="Link to this definition">thread_how_many_proc</a> &middot; <a href="#ParallelContext.thread_stat" title="Link to this definition">thread_stat</a> &middot; <a href="#ParallelContext.Threads" title="Link to this definition">Threads</a> &middot; <a href="#ParallelContext.threshold" title="Link to this definition">threshold</a> &middot; <a href="#ParallelContext.time" title="Link to this definition">time</a> &middot; <a href="#ParallelContext.timeout" title="Link to this definition">timeout</a> &middot; <a href="#ParallelContext.unpack" title="Link to this definition">unpack</a> &middot; <a href="#ParallelContext.upkpyobj" title="Link to this definition">upkpyobj</a> &middot; <a href="#ParallelContext.upkscalar" title="Link to this definition">upkscalar</a> &middot; <a href="#ParallelContext.upkstr" title="Link to this definition">upkstr</a> &middot; <a href="#ParallelContext.upkvec" title="Link to this definition">upkvec</a> &middot; <a href="#ParallelContext.userid" title="Link to this definition">userid</a> &middot; <a href="#ParallelContext.vtransfer_time" title="Link to this definition">vtransfer_time</a> &middot; <a href="#ParallelContext.wait_time" title="Link to this definition">wait_time</a> &middot; <a href="#ParallelContext.working" title="Link to this definition">working</a></dd></dl></p><section id="parallelcontext">
<span id="parcon"></span><h1>ParallelContext<a class="headerlink" href="#parallelcontext" title="Link to this heading"></a></h1>
<p>A video tutorial of parallelization in NEURON from the 2021 NEURON
summer webinar series is available <a class="reference internal" href="../../../../videos/neuron-course-2021.html#parallel-neuron-sims-2021-07-13"><span class="std std-ref">here</span></a>.</p>
<div class="toctree-wrapper compound">
</div>
<dl class="py class">
<dt class="sig sig-object py" id="ParallelContext">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ParallelContext</span></span><a class="headerlink" href="#ParallelContext" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc</span> <span class="pre">=</span> <span class="pre">h.ParallelContext()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc</span> <span class="pre">=</span> <span class="pre">h.ParallelContext(nhost)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>“Embarrassingly” parallel computations using a Bulletin board style
analogous to LINDA. (But see the <a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a>,
<a class="reference internal" href="parnet.html#ParallelNetManager" title="ParallelNetManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParallelNetManager</span></code></a> and <a class="reference internal" href="#paralleltransfer"><span class="std std-ref">Parallel Transfer</span></a> discussions.
Also see <a class="reference internal" href="../../../../hoc/modelspec/programmatic/network/parcon.html#subworld"><span class="std std-ref">SubWorld</span></a> for a way to simultaneously use
the bulletin board and network simulations involving global identifiers.)
Useful when doing weeks or months worth of
simulation runs each taking more than a second and where not much
communication is required.  Eg.  parameter sensitivity, and some forms
of optimization.  The underlying strategy is to keep all machines in a
PVM or <a class="reference internal" href="#parallelcontext-mpi"><span class="std std-ref">MPI</span></a>
virtual machine (eg.  workstation cluster) as busy as possible by
distinguishing between hosts (cpu’s) and tasks.  A task started by a
host stays on that host til it finishes.  The code that a host is
executing may submit other tasks and while waiting for them to finish
that host may start other tasks, perhaps one it is waiting for.
Early tasks tend to get done first
through the use of a tree shaped priority scheme.  We try to set things
up so that any cpu can execute any task.  The performance is good when
there are always tasks to perform.  In this case, cpu’s never are
waiting for other cpu’s to finish results but constantly take a task
from the bulletin board and put the result back onto the bulletin board.
Communication overhead is not bad if each task takes a second or more.</p>
<p>When using the Bulletin board with Python, the methods
<a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">submit()</span></code></a>, <a class="reference internal" href="#ParallelContext.context" title="ParallelContext.context"><code class="xref py py-meth docutils literal notranslate"><span class="pre">context()</span></code></a>, <a class="reference internal" href="#ParallelContext.pack" title="ParallelContext.pack"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pack()</span></code></a>, and <a class="reference internal" href="#ParallelContext.post" title="ParallelContext.post"><code class="xref py py-meth docutils literal notranslate"><span class="pre">post()</span></code></a>
have been augmented and <a class="reference internal" href="#ParallelContext.pyret" title="ParallelContext.pyret"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyret()</span></code></a> and <a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-meth docutils literal notranslate"><span class="pre">upkpyobj()</span></code></a> have been introduced
to allow a more Pythonic style. I.e. The executable
string for submit and context may be replaced by a Python callable that
returns a Python Object (retrieved with pyret), the args to submit, context, pack, and post
may be Python Objects, and a bulletin board message value which is a Python
Object may be retrieved with upkpyobj. At the end of the
following hoc parallelization and discussion the same example is
repeated as a Python parallelization. The only restriction is that any
python object arguments or return values must be pickleable (see
<a class="reference external" href="http://docs.python.org/library/pickle.html">http://docs.python.org/library/pickle.html</a>. As of this writing, hoc
objects are not pickleable.)</p>
<p>The simplest form of parallelization of a loop
from the users point of view is</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>

<span class="c1"># importing MPI or h.nrnmpi_init() must come before the first instantiation of ParallelContext()</span>
<span class="n">h</span><span class="o">.</span><span class="n">nrnmpi_init</span><span class="p">()</span>

<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;a function with no context that changes except its argument&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>

<span class="n">pc</span><span class="o">.</span><span class="n">runworker</span><span class="p">()</span> <span class="c1"># master returns immediately, workers in an</span>
               <span class="c1"># infinite loop running jobs from bulletin board</span>

<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">if</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>          <span class="c1"># use the serial form</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">f</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>                        <span class="c1"># use the bulleting board form</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>      <span class="c1"># scatter processes</span>
        <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>      <span class="c1"># any context needed by f had better be</span>
                             <span class="c1"># the same on all hosts</span>
    <span class="k">while</span> <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">():</span>      <span class="c1"># gather results</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">pc</span><span class="o">.</span><span class="n">pyret</span><span class="p">()</span>      <span class="c1"># the return value for the executed function</span>

<span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">pc</span><span class="o">.</span><span class="n">done</span><span class="p">()</span>                    <span class="c1"># tell workers to quit</span>
</pre></div>
</div>
<p>Several things need to be highlighted:</p>
<p>If a given task submits other tasks, only those child tasks
will be gathered by the working loop for that given task.
At this time the system groups tasks according to the parent task
and the pc instance is not used. See <a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.submit()</span></code></a> for
further discussion of this limitation. The safe strategy is always to
use the idiom:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>for i in range(n):
    pc.submit(...)          # scatter a set of tasks
while pc.working():         # gather them all
</pre></div>
</div>
<p>Earlier submitted tasks tend to complete before later submitted tasks, even
if they submit tasks themselves. Ie, A submitted
task has the same general priority as the parent task
and the specific priority of tasks with the same parent
is in submission order.
A free cpu always works on the
next unexecuted task with highest priority.</p>
<p>Each task manages a separate group of submissions
whose results are returned only to that task. Therefore you can
submit tasks which themselves submit tasks.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pc.working()</span></code> call checks to see if a result is ready. If so it returns
the unique system generated task id (a positive integer)
and the return value of the task
function is accessed via
the <code class="docutils literal notranslate"><span class="pre">pc.pyret()</span></code> function. The arguments to the function executed by the
submit call are also available. If all submissions have been computed and all
results have been returned, <code class="docutils literal notranslate"><span class="pre">pc.working()</span></code> returns 0. If results are
pending, working executes tasks from ANY ParallelContext until a
result is ready. This last feature keeps cpus busy but places stringent
requirements on how the user changes global context without
introducing bugs. See the discussion in <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> .</p>
<p>ParallelContext.working may not return results in the order of
submission.</p>
<p>Python code subsequent to pc.runworker() is executed only by the
master since that call returns immediately if the process is
the master and otherwise starts an infinite loop on each worker
which requests and executes submit tasks from ANY ParallelContext
instance. This is the standard way to seed the bulletin board with
submissions. Note that workers may also execute tasks that themselves
cause submissions. If subsidiary tasks call <code class="docutils literal notranslate"><span class="pre">pc.runworker()</span></code>, the call
returns immediately. Otherwise the task
it is working on would never complete!
The pc.runworker() function is also called for each worker after all code files
are read in and executed.</p>
<p>The basic organization of a simulation is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># setup which is exactly the same on every machine.</span>
<span class="c1"># ie declaration of all functions, procedures, setup of neurons</span>

<span class="n">pc</span><span class="o">.</span><span class="n">runworker</span><span class="p">()</span> <span class="c1"># to start the execute loop if this machine is a worker</span>

<span class="c1"># the master scatters tasks onto the bulletin board and gathers results</span>

<span class="n">pc</span><span class="o">.</span><span class="n">done</span><span class="p">()</span>
</pre></div>
</div>
<p>Issues having to do with context can become quite complex. Context
transfer from one machine to another should be as small as possible.
Don’t fall into the trap of a context transfer which takes longer
than the computation itself. Remember, you can do thousands of
c statements in the time it takes to transfer a few doubles.
Also, with a single cpu, it is often the case that statements
can be moved out of an innermost loop, but can’t be in a parallel
computation. eg.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pretend g is a Vector assigned earlier to conductances to test</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sec</span> <span class="ow">in</span> <span class="n">h</span><span class="o">.</span><span class="n">allsec</span><span class="p">():</span>
        <span class="n">sec</span><span class="o">.</span><span class="n">gnabar_hh</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">stim</span><span class="o">.</span><span class="n">amp</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">h</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>ie we only need to set gnabar_hh 20 times. But the first pass at
parallelization would look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">single_run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sec</span> <span class="ow">in</span> <span class="n">h</span><span class="o">.</span><span class="n">allsec</span><span class="p">():</span>
        <span class="n">sec</span><span class="o">.</span><span class="n">gnabar_hh</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">stim</span><span class="o">.</span><span class="n">amp</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">h</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
   <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">single_run</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>


<span class="k">while</span> <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">():</span> <span class="k">pass</span>
</pre></div>
</div>
<p>and we take the hit of repeated evaluation of gnabar_hh.
A run must be quite lengthy to amortize this overhead.</p>
<p>To run under MPI, be sure to include the <code class="docutils literal notranslate"><span class="pre">h.nrnmpi_init()</span></code> and then
launch your script via, e.g. <code class="docutils literal notranslate"><span class="pre">mpiexec</span> <span class="pre">-n</span> <span class="pre">4</span> <span class="pre">python</span> <span class="pre">myscript.py</span></code>. NEURON
also supports running via the PVM (parallel virtual machine), but the launch
setup is different. If you do not have mpi4py and you have not exported
the NEURON_INIT_MPI=1 environment variable then you can use the
h.nrnmpi_init() method as long as that is executed prior to the first
instantiation of ParallelContext.</p>
<p>The exact same Python files should exist in the same relative locations
on all host machines.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Not much checking for correctness or help in finding common bugs.</p>
<p>The best sanity test of a working mpi environment is testmpi.py</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">h</span><span class="o">.</span><span class="n">nrnmpi_init</span><span class="p">()</span>

<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;I am </span><span class="si">{</span><span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<p>which gives ( the output lines are in indeterminate order)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpiexec</span> <span class="o">-</span><span class="n">n</span> <span class="mi">3</span> <span class="n">python</span> <span class="n">testmpi</span><span class="o">.</span><span class="n">py</span>
<span class="n">numprocs</span><span class="o">=</span><span class="mi">3</span>
<span class="n">I</span> <span class="n">am</span> <span class="mi">0</span> <span class="n">of</span> <span class="mi">3</span>
<span class="n">I</span> <span class="n">am</span> <span class="mi">1</span> <span class="n">of</span> <span class="mi">3</span>
<span class="n">I</span> <span class="n">am</span> <span class="mi">2</span> <span class="n">of</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nhost">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nhost</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nhost" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.nhost()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns number of host neuron processes (master + workers).
If MPI (or PVM) is not being used then nhost == 1 and all ParallelContext
methods still work properly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">if</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="k">else</span><span class="p">:</span>
   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
      <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>


   <span class="k">while</span> <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">():</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">userid</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">pyret</span><span class="p">())</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prior to NEURON 7.6, this function returned a value of type <code class="docutils literal notranslate"><span class="pre">float</span></code>;
in more recent versions of NEURON, the return is an <code class="docutils literal notranslate"><span class="pre">int</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.id">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">id</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.id" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">myid</span> <span class="pre">=</span> <span class="pre">pc.id()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The ihost index which ranges from 0 to pc.nhost()-1 . Otherwise
it is 0. The master machine always has an pc.id() == 0.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For MPI, the pc.id() is the rank from
MPI_Comm_rank.  For PVM the pc.id() is the order that the HELLO message was
received by the master.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prior to NEURON 7.6, this function returned a value of type <code class="docutils literal notranslate"><span class="pre">float</span></code>;
in more recent versions of NEURON, the return is an <code class="docutils literal notranslate"><span class="pre">int</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.mpiabort_on_error">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">mpiabort_on_error</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.mpiabort_on_error" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">oldflag</span> <span class="pre">=</span> <span class="pre">pc.mpiabort_on_error(newflag)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Normally, when running with MPI, a hoc error causes a call to MPI_Abort
so that all processes can be notified to exit cleanly without
any processes hanging. (e.g. waiting for a communicator
collective to complete). The call to MPI_Abort can be avoided by
calling this method with newflag = 0. This occurs automatically
when <a class="reference internal" href="../../../programming/dynamiccode.html#execute1" title="execute1"><code class="xref py py-func docutils literal notranslate"><span class="pre">execute1()</span></code></a> is used.  The method returns the previous
value of the flag.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.submit">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">submit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.submit" title="Link to this definition"></a></dt>
<dd><p>Syntax:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">pc.submit(python_callable,</span> <span class="pre">arg1,</span> <span class="pre">...)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.submit(userid,</span> <span class="pre">..as</span> <span class="pre">above..)</span></code></p>
</div></blockquote>
<dl>
<dt>Description:</dt><dd><p>Submits statement for execution by any host. Submit returns the userid not the
system generated global id of the task.
However when the task is executed, the <code class="xref py py-data docutils literal notranslate"><span class="pre">hoc_ac_</span></code> variable
is set to this unique id (positive integer) of the task.
This unique id is returned by <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> .</p>
<p>If the first argument to submit is a non-negative integer
then args are not saved and when the id for this
task is returned by <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a>,
that non-negative integer can be retrieved with
<a class="reference internal" href="#ParallelContext.userid" title="ParallelContext.userid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.userid()</span></code></a></p>
<p>If there is no explicit userid, then the args (after the function name)
are saved locally and can be unpacked when the corresponding working
call returns. A local userid (unique only for this ParallelContext)
is generated and returned by the submit call and is also retrieved with
<a class="reference internal" href="#ParallelContext.userid" title="ParallelContext.userid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.userid()</span></code></a> when the corresponding working call returns.
This is very useful in associating a particular parameter vector with
its return value and avoids the necessity of explicitly saving them
or posting them. If they are not needed and you do not wish to
pay the overhead of storage, supply an explicit userid.
Unpacking args must be done in the same order and have the same
type as the args of the “function_name”. They do not have to be unpacked.
Saving args is time efficient since it does not imply extra communication
with the server.</p>
<p>Arguments may be any pickleable objects (The NEURON <a class="reference internal" href="../../../programming/math/vector.html#Vector" title="Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> is
pickleable, and  most built-in Python objects and user-defined
classes are pickleable). The callable is executed on some indeterminate MPI
host. The return value is a Python object and may be retrieved with
<a class="reference internal" href="#ParallelContext.pyret" title="ParallelContext.pyret"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pyret()</span></code></a>.  Python object arguments
may be retrieved with <a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-func docutils literal notranslate"><span class="pre">upkpyobj()</span></code></a>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a>,
<a class="reference internal" href="#ParallelContext.retval" title="ParallelContext.retval"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.retval()</span></code></a>, <a class="reference internal" href="#ParallelContext.userid" title="ParallelContext.userid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.userid()</span></code></a>,
<a class="reference internal" href="#ParallelContext.pyret" title="ParallelContext.pyret"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.pyret()</span></code></a></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>submit does not return the system generated unique id of the task but
either the first arg (must be a positive integer to be a userid) or
a locally (in this ParallelContext) generated userid which starts at 1.</p>
<p>A task should gather the results of all the tasks it submits before
scattering other tasks even if scattering with different ParallelContext
instances. This is because results are grouped by parent task id’s
instead of (parent task id, pc instance). Thus the following idiom
needs extra user defined info to distinguish between pc1 and pc2 task
results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">pc1</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">pc2</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">pc1</span><span class="o">.</span><span class="n">working</span><span class="p">()</span> <span class="o">...</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">pc2</span><span class="o">.</span><span class="n">working</span><span class="p">()</span> <span class="o">...</span>
</pre></div>
</div>
<p>since pc1.working() may get a result from a pc2 submission
If this behavior is at all inconvenient, I will change the semantics
so that pc1 results only are gathered by pc1.working calls and by no
others.</p>
<p>Searching for the proper object context (pc.submit(object, …) on the
host executing the submitted task is linear in the
number of objects of that type.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.working">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">working</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.working" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">id</span> <span class="pre">=</span> <span class="pre">pc.working()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns 0 if there are no pending submissions which were
submitted by the current task.
(see bug below with regard to the distinction between the current
task and a ParallelContext instance). Returns the id of a previous pc.submit
which has completed
and whose results from that computation are ready for retrieval.</p>
<p>While there are pending submissions and results are not ready, pending
submissions from any ParallelContext from any host are calculated.
Note that returns of completed submissions are not necessarily in the
order that they were made by pc.submit.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="k">break</span>
    <span class="c1"># gather results of previous pc.submit calls</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">pc</span><span class="o">.</span><span class="n">pyret</span><span class="p">())</span>
</pre></div>
</div>
<p>Note that if the submission did not have an explicit userid then
all the arguments of the executed function may be unpacked.</p>
<p>It is essential to emphasize that when
a task calls pc.working, while it is waiting for a result, it may
execute any number of other tasks and unless care is taken to
understand the meaning of “task context” and guarantee that
context after the working call is the same as the context before the
working call, SUBTLE ERRORS WILL HAPPEN more or less frequently
and indeterminately. For example consider the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
   <span class="o">...</span> <span class="n">write</span> <span class="n">some</span> <span class="n">values</span> <span class="n">to</span> <span class="n">some</span> <span class="k">global</span> <span class="n">variables</span> <span class="o">...</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
   <span class="c1"># when g is executed on another host it will not in general</span>
   <span class="c1"># see the same global variable values you set above.</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">()</span> <span class="c1"># get back result of execution of g(...)</span>
   <span class="c1"># now the global variables may be different than what you</span>
   <span class="c1"># set above. And not because g changes them but perhaps</span>
   <span class="c1"># because the host executing this task started executing</span>
   <span class="c1"># another task that called f which then wrote DIFFERENT values</span>
   <span class="c1"># to these global variables.</span>
</pre></div>
</div>
<p>I only know one way around this problem. Perhaps there are other and
better ways.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
   <span class="nb">id</span> <span class="o">=</span> <span class="n">hoc_ac_</span>
   <span class="c1"># write some values to some global variables ...</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">the</span><span class="p">,</span> <span class="k">global</span><span class="p">,</span> <span class="n">variables</span><span class="p">)</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">()</span>
   <span class="n">pc</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
   <span class="c1"># unpack the info back into the global variables</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.submit()</span></code></a>,
<a class="reference internal" href="#ParallelContext.retval" title="ParallelContext.retval"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.retval()</span></code></a>, <a class="reference internal" href="#ParallelContext.userid" title="ParallelContext.userid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.userid()</span></code></a>,
<a class="reference internal" href="#ParallelContext.pyret" title="ParallelContext.pyret"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.pyret()</span></code></a></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Submissions are grouped according to parent task id and not by
parallel context instance. If suggested by actual experience, the
grouping will be according to the pair (parent task id, parallel
context instance). Confusion arises only in the case where a task
submits jobs  with one pc and fails to gather them before
submitting another group of jobs with another pc. See the bugs section
of <a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.submit()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.retval">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">retval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.retval" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">scalar</span> <span class="pre">=</span> <span class="pre">pc.retval()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The return value of the function executed by the task gathered by the
last <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> call.
If the statement form of the submit is used then the return value
is the value of <code class="xref py py-data docutils literal notranslate"><span class="pre">hoc_ac_</span></code> when the statement completes on the executing host.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Use <a class="reference internal" href="#ParallelContext.pyret" title="ParallelContext.pyret"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.pyret()</span></code></a> for tasks submitted as Python callables; do not use
<code class="docutils literal notranslate"><span class="pre">pc.retval()</span></code> which only works for tasks submitted as HOC strings.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.pyret">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">pyret</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.pyret" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">python_object</span> <span class="pre">=</span> <span class="pre">pc.pyret()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>If a task is submitted defined as a Python callable then the return
value can be any Python object and can only be retrieved with pyret().
This function can only be called once for the task result gathered
by the last <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> call.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.userid">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">userid</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.userid" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">scalar</span> <span class="pre">=</span> <span class="pre">pc.userid()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The return value of the corresponding submit call.
The value of the userid is either the
first argument (if it was a non-negative integer)
of the submit call or else it is a positive integer unique only to
this ParallelContext.</p>
<p>See <a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.submit()</span></code></a> with regard to retrieving the original
arguments of the submit call corresponding to the working return.</p>
<p>Can be useful in organizing results according to an index defined during
submission.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.runworker">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">runworker</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.runworker" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.runworker()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The master host returns immediately. Worker hosts start an
infinite loop of requesting tasks for execution.</p>
<p>The basic style is that the master and each host execute the
same code up til the pc.runworker call and that code sets up
all the context that is required to be identical on all hosts so
that any host can run any task whenever the host requests something
todo. The latter takes place in the runworker loop and when a task
is waiting for a result in a <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> call.
Many parallel processing bugs
are due to inconsistent context among hosts and those bugs
can be VERY subtle. Tasks should not change the context required
by other tasks without extreme caution. The only way I know how
to do this safely
is to store and retrieve a copy of
the authoritative context on the bulletin board. See
<a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> for further discussion in this regard.</p>
<p>The runworker method is called automatically for each worker after
all files have been read in and executed — i.e. if the user never
calls it explicitly from Python. Otherwise the workers would exit since
the standard input is at the end of file for workers.
This is useful in those cases where
the only distinction between master and workers is that code
executed from the gui or console.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.done">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">done</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.done" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.done()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Sends the QUIT message to all worker hosts. Those NEURON processes then
exit. The master waits til all worker output has been transferred to
the master host.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.context">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">context</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.context" title="Link to this definition"></a></dt>
<dd><p>Syntax:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">pc.context(python_callable,</span> <span class="pre">arg1,</span> <span class="pre">...)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.context(userid,</span> <span class="pre">..as</span> <span class="pre">above..)</span></code></p>
</div></blockquote>
<dl>
<dt>Description:</dt><dd><p>The arguments have the same semantics as those of <a class="reference internal" href="#ParallelContext.submit" title="ParallelContext.submit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.submit()</span></code></a>.
The function or statement is executed on every worker host
but is not executed on the master. pc.context can only be
called by the master. The workers will execute the context statement
when they are idle or have completed their current task.
It probably only makes sense for the python_callable to return None.</p>
<p>There is no return in the
sense that <a class="reference internal" href="#ParallelContext.working" title="ParallelContext.working"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.working()</span></code></a> does not return when one
of these tasks completes.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is not clear if it would be useful to generalize
the semantics to
the case of executing on every host except the
host that executed the pc.context call.
(strictly, the host would execute the task
when it requests something to do.
i.e. in a working loop or in a worker’s infinite work loop.)
The simplest and safest use of this method is if it is called by the master
when all workers are idle.</p>
<p>This method was introduced in an attempt to get a parallel
multiple run fitter which worked in an interactive gui setting.
As such it increases safety but is not bulletproof since
there is no guarantee that the user doesn’t change a global
variable that is not part of the fitter. It is also difficult
to write safe code that invariably makes all the relevant worker
context identical to the master.  An example of a common bug
is to remove a parameter from the parameter list and then
call save_context(). Sure enough, the multiple run fitters
on all the workers will no longer use that parameter, but
the global variables that depend on the parameter may be
different on different hosts and they will now stay different!
One fix is to call save_context() before the removal of the
parameter from the list and save_context() after its removal.
But the inefficiency is upsetting. We need a better automatic
mirroring method.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.post">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">post</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.post" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.post(key)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.post(key,</span> <span class="pre">...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Post the message with the address key, (key may be a string or scalar),
and a body consisting of any number of <a class="reference internal" href="#ParallelContext.pack" title="ParallelContext.pack"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.pack()</span></code></a> calls since
the last post, and any number of arguments of type scalar, Vector, strdef
or Python object.</p>
<p>Later unpacking of the message body must be done in the same order as
this posting sequence.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.pack" title="ParallelContext.pack"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.pack()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.take">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">take</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.take" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.take(key)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.take(key,</span> <span class="pre">...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Takes the message with key from the bulletin board. If the key does
not exist then the call blocks. Two processes can never take the same
message (unless someone posts it twice). The key may be a string or scalar.
Unpacking the message must take place in the same order as the packing
and must be complete before the next bulletin board operation.
(at which time remaining message info will be discarded)
It is not required to unpack the entire message, but later items cannot
be retrieved without unpacking earlier items first. Optional arguments
get the first unpacked values. Scalar, Vectors, and strdef may be
unpacked. Scalar arguments must be pointers to
a variable. eg  <code class="docutils literal notranslate"><span class="pre">_ref_x</span></code>. Unpacked Vectors will be resized to the
correct size of the vector item of the message.
To unpack Python objects, <a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-func docutils literal notranslate"><span class="pre">upkpyobj()</span></code></a> must be used.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.upkstr" title="ParallelContext.upkstr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkstr()</span></code></a>, <a class="reference internal" href="#ParallelContext.upkscalar" title="ParallelContext.upkscalar"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkscalar()</span></code></a>,
<a class="reference internal" href="#ParallelContext.upkvec" title="ParallelContext.upkvec"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkvec()</span></code></a>, <a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkpyobj()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.look">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">look</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.look" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">boolean</span> <span class="pre">=</span> <span class="pre">pc.look(key)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">boolean</span> <span class="pre">=</span> <span class="pre">pc.look(key,</span> <span class="pre">...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Like <a class="reference internal" href="#ParallelContext.take" title="ParallelContext.take"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.take()</span></code></a> but does not block or remove message
from bulletin board. Returns 1 if the key exists, 0 if the key does
not exist on the bulletin board. The message associated with the
key (if the key exists) is available for unpacking each time
pc.look returns 1.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.look_take" title="ParallelContext.look_take"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.look_take()</span></code></a>, <a class="reference internal" href="#ParallelContext.take" title="ParallelContext.take"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.take()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.look_take">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">look_take</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.look_take" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">boolean</span> <span class="pre">=</span> <span class="pre">pc.look_take(key,</span> <span class="pre">...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Like <a class="reference internal" href="#ParallelContext.take" title="ParallelContext.take"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.take()</span></code></a> but does not block. The message is
removed from the bulletin board and two processes will never receive
this message. Returns 1 if the key exists, 0 if the key does not
exist on the bulletin board. If the key exists, the message can
be unpacked.</p>
<p>Note that a look followed by a take is <em>NOT</em> equivalent to look_take.
It can easily occur that another task might take the message between
the look and take and the latter will then block until some other
process posts a message with the same key.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.take" title="ParallelContext.take"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.take()</span></code></a>, <a class="reference internal" href="#ParallelContext.look" title="ParallelContext.look"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.look()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.pack">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">pack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.pack" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.pack(...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Append arguments consisting of scalars, Vectors, strdefs,
and pickleable Python objects into a message body
for a subsequent post.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.post" title="ParallelContext.post"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.post()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.unpack">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">unpack</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.unpack" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.unpack(...)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Extract items from the last message retrieved with
take, look, or look_take. The type and sequence of items retrieved must
agree with the order in which the message was constructed with post
and pack.
Note that scalar items must be retrieved with pointer syntax as in
<code class="docutils literal notranslate"><span class="pre">soma(0.3).hh._ref_gnabar</span></code>
To unpack Python objects, <a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-meth docutils literal notranslate"><span class="pre">upkpyobj()</span></code></a> must be used.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.upkscalar" title="ParallelContext.upkscalar"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkscalar()</span></code></a>
<a class="reference internal" href="#ParallelContext.upkvec" title="ParallelContext.upkvec"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkvec()</span></code></a>, <a class="reference internal" href="#ParallelContext.upkstr" title="ParallelContext.upkstr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkstr()</span></code></a>
<a class="reference internal" href="#ParallelContext.upkpyobj" title="ParallelContext.upkpyobj"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.upkpyobj()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.upkscalar">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">upkscalar</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.upkscalar" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">pc.upkscalar()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return the scalar item which must be the next item in the unpacking
sequence of the message retrieved by the previous take, look, or look_take.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.upkstr">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">upkstr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.upkstr" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">str</span> <span class="pre">=</span> <span class="pre">pc.upkstr(str)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Copy the next item in the unpacking
sequence into str and return that strdef.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">str</span></code> here is a <code class="docutils literal notranslate"><span class="pre">strdef</span></code> not a Python string. One may be created via e.g. <code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">=</span> <span class="pre">h.ref('')</span></code>; the stored string
can then be accessed via <code class="docutils literal notranslate"><span class="pre">s[0]</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.upkvec">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">upkvec</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.upkvec" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">vec</span> <span class="pre">=</span> <span class="pre">pc.upkvec()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">vec</span> <span class="pre">=</span> <span class="pre">pc.upkvec(vecsrc)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Copy the next item in the unpacking
sequence into vecsrc (if that arg exists, it will be resized if necessary).
If the arg does not exist return a new <a class="reference internal" href="../../../programming/math/vector.html#Vector" title="Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.upkpyobj">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">upkpyobj</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.upkpyobj" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">python_object</span> <span class="pre">=</span> <span class="pre">pc.upkpyobj()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return a reference to the (copied via pickling/unpickling)
Python object which must be the next item in the unpacking
sequence of the message retrieved by the previous take, look, or look_take.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.time" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">st</span> <span class="pre">=</span> <span class="pre">pc.time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns a high resolution elapsed wall clock time on the processor
(units of seconds) since an arbitrary time in the past.
Normal usage is</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">st</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="o">...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">st</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A wrapper for MPI_Wtime when MPI is used. When PVM is used, the return
value is <code class="samp docutils literal notranslate"><span class="pre">clock_t</span> <span class="pre">times(struct</span> <span class="pre">tms</span> <em><span class="pre">*buf</span></em><span class="pre">)/100</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.wait_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">wait_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.wait_time" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">pc.wait_time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
on a worker spent waiting for a message from the master. For the master,
it is the amount of time in the pc.take calls that was spent waiting.</p>
<p>To determine the time spent exchanging spikes during a simulation, use
the idiom:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wait</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">wait_time</span><span class="p">()</span>
<span class="n">pc</span><span class="o">.</span><span class="n">psolve</span><span class="p">(</span><span class="n">tstop</span><span class="p">)</span>
<span class="n">wait</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">wait_time</span><span class="p">()</span> <span class="o">-</span> <span class="n">wait</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.step_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">step_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.step_time" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">pc.step_time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
on a cpu spent integrating equations, checking thresholds, and delivering
events. It is essentially pc.integ_time + pc.event_time.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.send_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">send_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.send_time" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">pc.send_time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
on a cpu spent directing source gid spikes arriving on the target gid
to the proper PreSyn.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.event_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">event_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.event_time" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">pc.event_time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
on a cpu spent checking thresholds and delivering spikes. Note that
pc.event_time() + pc.send_time() will include all spike related time but
NOT the time spent exchanging spikes between cpus.
(Currently only for fixed step)</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.integ_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">integ_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.integ_time" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">=</span> <span class="pre">pc.integ_time()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
on a cpu spent integrating equations. (currently only for fixed step)</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.vtransfer_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">vtransfer_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.vtransfer_time" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">transfer_exchange_time</span> <span class="pre">=</span> <span class="pre">pc.vtransfer_time()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">splitcell_exchange_time</span> <span class="pre">=</span> <span class="pre">pc.vtransfer_time(1)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">reducedtree_computation_time</span> <span class="pre">=</span> <span class="pre">pc.vtransfer_time(2)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The amount of time (seconds)
spent transferring and waiting for voltages or matrix elements.
The <a class="reference internal" href="#ParallelContext.integ_time" title="ParallelContext.integ_time"><code class="xref py py-func docutils literal notranslate"><span class="pre">integ_time()</span></code></a> is reduced by transfer and splitcell exchange times.</p>
<p>splitcell_exchange_time includes the reducedtree_computation_time.</p>
<p>reducedtree_computation_time refers to the extra time used by the
<a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a> backbone_style 1 and 2 methods between
send and receive of matrix information. This amount is also included
in the splitcell_exchange_time.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.mech_time">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">mech_time</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.mech_time" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.mech_time()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">mechanism_time</span> <span class="pre">=</span> <span class="pre">pc.mech_time(i)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>With no args initializes the mechanism time to 0. The next run will
record the computation time for BREAKPOINT and SOLVE statements of each
mechanism used in thread 0. When the index arg is present, the computation
time taken by the mechanism with that index is returned.
The index value is the internal
mechanism type index, not the index of the MechanismType.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="../mechtype.html#MechanismType.internal_type" title="MechanismType.internal_type"><code class="xref py py-meth docutils literal notranslate"><span class="pre">MechanismType.internal_type()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<section id="implementation-notes">
<h2>Implementation Notes<a class="headerlink" href="#implementation-notes" title="Link to this heading"></a></h2>
<dl>
<dt>Description:</dt><dd><p>Some of these notes are PVM specific.</p>
<p>With the following information you may be encouraged to provide
a more efficient implementation. You may also see enough information
here to decide that this implementation is about as good as can be
expected in the context of your problem.</p>
<p>The master NEURON process contains the server for the bulletin board system.
Communication between normal Python  code executing on the master NEURON
process and the
server is direct with no overhead except packing and unpacking
messages and manipulating the send and receive buffers with pvm commands.
The reason I put the server into the master process is twofold.
1) While the master is number crunching, client messages are still
promptly dealt with. I noticed that when neuron was cpu bound, a separate
server process did not respond to requests for about a tenth of a second.
2) No context switching between master process and server.
If pvm is not running, a local implementation of the server is used
which has even less overhead than pvm packing and unpacking.</p>
<p>Clients (worker processes) communicate with the bulletin board server
(in the master machine) with pvm commands pvm_send and pvm_recv.
The master process is notified of asynchronous events via the SIGPOLL
signal. Unfortunately this is often early since a pvm message often
consists of several of these asynchronous events and my experience
so far is that (pvm_probe(-1,-1) &gt; 0) is not always true even after
the last of this burst of signals. Also SIGPOLL is not available
except under UNIX. However SIGPOLL is only useful on the master
process and should not affect performance with regard to whether a
client is working under Win95, NT, or Linux. So even with SIGPOLL
there must be software polling on the server and this takes place
on the next execute() call in the interpreter. (an execute call
takes place when the body of every for loop, if statement, or
function/procedure call is executed.) In the absence of a SIGPOLL
signal this software polling takes place every POLLDELAY=20
executions. Of course this is too seldom in the case of
fadvance calls with a very large model, and too often in the case
of for i=1,100000 x+=i. Things are generally ok if the
message at the end of a run says that the amount of time spent
waiting for something to do is small compared to the amount of time
spent doing things. Perhaps a timer would help.</p>
<p>The bulletin board server consists of several lists implemented with
the STL (Standard Template Library) which makes for reasonably fast
lookup of keys. ie searching is not proportional to the size of the
list but proportional to the log of the list size.</p>
<p>Posts go into the message list ordered by key (string order).
They stay there until taken with look_take or take.
Submissions go into a work list ordered by id and a todo list of id’s
by priority. When a host requests something to do, the highest priority
(first in the list) id is taken off the todo list. When done, the id goes
onto a results list ordered by parent id. When working is called
and a results list has an id with the right parent id, the
id is removed from the results list and the (id, message) pair
is removed from the work list.</p>
<p>If args are saved (no explicit userid in the submit call), they are
stored locally and become the active buffer on the corresponding
working return. The saving is in an STL map associated with userid.
The data itself is not copied but neither is it released until
the next usage of the receive buffer after the working call returns.</p>
</dd>
</dl>
<hr class="docutils" />
</section>
<section id="mpi">
<span id="parallelcontext-mpi"></span><h2>MPI<a class="headerlink" href="#mpi" title="Link to this heading"></a></h2>
<dl>
<dt>Description:</dt><dd><p>If MPI is already installed, lucky you. You should ask the installer
for help.</p>
<p>Here is how I got it going on a 24 cpu beowulf cluster and
a dual processor Mac OSX G5. The cluster consisted of 12 dual processor
nodes named node0 to node11 and a master. From the outside world you
could only login to the master using ssh and from there to any of the nodes
you also had to use ssh. For a second opinion see
<a class="reference internal" href="lyttonmpi.html"><span class="doc">Bill Lytton’s notes on installing MPI</span></a>.</p>
<ol class="arabic simple">
<li><p>Figure out how to login to a worker without typing a password.</p></li>
</ol>
<p>ie. do not go on unless you can
<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">node1</span></code> or <code class="docutils literal notranslate"><span class="pre">rsh</span> <span class="pre">node1</span></code>. If the former works then you must
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">RSHCOMMAND=ssh</span></code> before building the MPICH version of MPI since
that information is compiled into one of the files. It’s too late to set
it after MPICH has been built.</p>
<p>On the Beowulf cluster master I did:
<code class="docutils literal notranslate"><span class="pre">ssh-keygen</span> <span class="pre">-t</span> <span class="pre">rsa</span></code>
and just hit return three times (once to use the default file location
and twice to specify and confirm an empty password).
Then I did a
<code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">$HOME/.ssh</span></code> and copied the id_rsa.pub file to authorized_keys.
Now I could login to any node without using a password.</p>
<p>On the OSX machine I did the same thing but had to also check the
SystemPreferences/Internet&amp;Network Sharing/Services/RemoteLogin box.</p>
<ol class="arabic simple" start="2">
<li><p>install MPI</p></li>
</ol>
<p>I use <a class="reference external" href="http://www-unix.mcs.anl.gov/mpi/mpich/downloads/mpich.tar.gz">http://www-unix.mcs.anl.gov/mpi/mpich/downloads/mpich.tar.gz</a>
which on extraction ended up in $HOME/mpich-1.2.7. I built on
osx with</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export RSHCOMMAND=ssh
./configure --prefix=`pwd`/powerpc --with-device=ch_p4
make
make install
</pre></div>
</div>
<p>and the same way on the beowulf cluster but with i686 instead of powerpc.
I then added $HOME/mpich-1.2.7/powerpc/bin to my PATH because the
NEURON configuration process will need to find mpicc and mpicxx
and we will eventually be using mpirun.</p>
<p>Note: some systems may have a
different implementation of MPI already installed and in that
implementation the c++ compiler
may be called mpic++. If that is in your path, then you will need to
go to $HOME/mpich-1.2.7/powerpc/bin and
<code class="docutils literal notranslate"><span class="pre">ln</span> <span class="pre">-s</span> <span class="pre">mpicxx</span> <span class="pre">mpic++</span></code>. This will prevent NEURON’s configure from becoming
confused and deciding to use mpicc from one MPI version and mpic++ from another!
ie. configure looks first for mpic++ and only if it does not find it does
it try mpicxx.</p>
<p>You can gain some confidence if you go to mpich-1.2.7/examples/basic and
test with</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>make hello++
mpirun -np 2 hello++
</pre></div>
</div>
<p>If this fails on the mac, you may need a machine file with the proper
name that is indicated at the end of the $HOME/.ssh/authorized_keys file.
In my case, since ssh-keygen called my machine Michael-Hines-Computer-2.local
I have to use</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{mpirun -machinefile $HOME/mpifile -np 2 hello++
</pre></div>
</div>
<p>where $HOME/mpifile has the single line</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Michael-Hines-Computer-2.local
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>build NEURON using the –with-paranrn argument.</p></li>
</ol>
<p>On the beowulf my neuron
sources were in $HOME/neuron/nrn and interviews was installed in
$HOME/neuron/iv and I decided to build in a separate object directory called
$HOME/neuron/mpi-gcc2.96 so I created the latter directory, cd’d to it
and used</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>../nrn/configure --prefix=`pwd` --srcdir=../nrn --with-paranrn
</pre></div>
</div>
<p>On the mac, I created a $HOME/neuron/withmpi directory and configured with</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>../nrn/configure --prefix=`pwd` --srcdir=../nrn --with-paranrn \
--enable-carbon --with-iv=/Applications/NEURON-5.8/iv
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>test by going to $HOME/neuron/nrn/src/parallel and trying</p></li>
</ol>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>mpirun -np 2  ~/neuron/withmpi/i686/bin/nrniv -mpi test0.hoc
</pre></div>
</div>
<p>You should get an output similar to</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>nrnmpi_init(): numprocs=2 myid=0
NEURON -- Version 5.8 2005-8-22 19:58:19 Main (52)
by John W. Moore, Michael Hines, and Ted Carnevale
Duke and Yale University -- Copyright 1984-2005

loading membrane mechanisms from i686/.libs/libnrnmech.so
Additional mechanisms from files

hello from id 0 on NeuronDev

        0
bbs_msg_cnt_=0 bbs_poll_cnt_=6667 bbs_poll_=93
        0
hello from id 1 on NeuronDev

[hines@NeuronDev parallel]$
</pre></div>
</div>
<p>5) If your machine is a cluster, list the machine names in a file
(on the beowulf cluster $HOME/mpi32 has the contents</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>node0
...
node11
</pre></div>
</div>
<p>)
and I use the mpirun command</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>mpirun -machinefile $HOME/mpi32 -np 24 \
    /home/hines/neuron/mpi*6/i686/bin/nrniv -mpi test0.hoc
</pre></div>
</div>
<p>On my mac, for some bizarre reason known only to the tiger creators,
the mpirun requires a machinefile with the line</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Michael-Hines-Computer-2.local
</pre></div>
</div>
</dd>
</dl>
<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.mpi_init">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">mpi_init</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.mpi_init" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">h.nrnmpi_init()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Initializes MPI if it has not already been initialized; mpi4py can
also be used to intialize MPI.
Only required if:</p>
<p>launched python and mpi4py not used and NEURON_INIT_MPI=1
environment varialble has not been exported.</p>
<p>launched nrniv without -mpi argument.</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">nrnmpi_init</span></code> turns off gui for all ranks &gt; 0, do not <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">neuron</span> <span class="pre">import</span> <span class="pre">gui</span></code>
beforehand.</p>
<p>The mpi_init method name was removed from ParallelContext and replaced
with the HocTopLevelInterpreter method nrnmpi_init() because MPI
must be initialized prior to the first instantiation of ParallelContext.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.barrier">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">barrier</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.barrier" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">waittime</span> <span class="pre">=</span> <span class="pre">pc.barrier()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Does an MPI_Barrier and returns the wait time at the barrier.  Execution
resumes only after all process reach this statement.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.allreduce">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">allreduce</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.allreduce" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">result</span> <span class="pre">=</span> <span class="pre">pc.allreduce(value,</span> <span class="pre">type)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.allreduce(src_dest_vector,</span> <span class="pre">type)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Type is 1, 2, or 3 and the every host gets a
result as sum over all value, maximum
value, or minimum value respectively</p>
<p>If the first arg is a <a class="reference internal" href="../../../programming/math/vector.html#Vector" title="Vector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Vector</span></code></a> the reduce is done element-wise. ie
min of each rank’s v[0] returned in each rank’s v[0], etc. Note that
each vector must have the same size.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.allgather">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">allgather</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.allgather" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.allgather(value,</span> <span class="pre">result_vector)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Every host gets the value from every other host. The value from a host id
is in the id’th element of the vector. The vector is resized to size
pc.nhost.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.alltoall">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">alltoall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.alltoall" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.alltoall(vsrc,</span> <span class="pre">vcnts,</span> <span class="pre">vdest)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Analogous to MPI_Alltoallv(…). vcnts must be of size pc.nhost and
vcnts.sum must equal the size of vsrc.
For host i, vcnts[j] elements of
vsrc are sent to host j beginning at the index vcnts.sum(0,j-1).
On host j, those elements are put into vdest beginning at the location
after the elements received from hosts 0 to i-1.
The vdest is resized to the number of elements received.
Note that vcnts are generally different for different hosts. If you need
to know how many came from what host, use the idiom
<code class="docutils literal notranslate"><span class="pre">pc.alltoall(vcnts,</span> <span class="pre">one,</span> <span class="pre">vdest)</span></code> where one is a vector filled with 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># assume vsrc is a sorted Vector with elements ranging from 0 to tstop</span>
<span class="c1"># then the following is a parallel sort such that vdest is sorted on</span>
<span class="c1"># host i and for i &lt; j, all the elements of vdest on host i are &lt;</span>
<span class="c1"># than all the elements on host j.</span>
<span class="n">vsrc</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="n">cnts</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">Vector</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">())</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">tvl</span>
  <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">x</span><span class="p">:</span>
      <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="n">cnts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>

<span class="n">pc</span><span class="o">.</span><span class="n">alltoall</span><span class="p">(</span><span class="n">vsrc</span><span class="p">,</span> <span class="n">cnts</span><span class="p">,</span> <span class="n">vdest</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.py_alltoall">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">py_alltoall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.py_alltoall" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">destlist</span> <span class="pre">=</span> <span class="pre">pc.py_alltoall(srclist)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Analogous to MPI_Alltoallv(…).
The srclist must be a Python list of nhost pickleable Python objects.
(Items with value None are allowed).
The ith object is communicated to the ith host. the return value is
a Python list of nhost items where the ith item was communicated
by the ith host. This is a collective operation, so all hosts must
participate.</p>
<p>An optional second integer argument &gt; 0 specifies the initial source
pickle buffer size in bytes. The default size is 100k bytes. The size
will grow by approximately doubling when needed.</p>
<p>If the optional second argument is -1, then no transfers will be made
and return value will be (src_buffer_size, dest_buffer_size) of the
pickle buffers which would be needed for sending and receiving.</p>
</dd>
</dl>
<p>Example:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">h</span><span class="o">.</span><span class="n">nrnmpi_init</span><span class="p">()</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">nhost</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>

<span class="c1">#Keep host output from being intermingled.</span>
<span class="c1">#Not always completely successful.</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="k">def</span> <span class="nf">serialize</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nhost</span><span class="p">):</span>
        <span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">==</span> <span class="n">rank</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">r</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">rank</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nhost</span><span class="p">)]</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;source data&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">serialize</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_alltoall</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;destination data&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">serialize</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">runworker</span><span class="p">()</span>
<span class="n">pc</span><span class="o">.</span><span class="n">done</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpirun -n 4 python parcon.py
numprocs=4
source data
0 [(0, 0), (0, 1), (0, 2), (0, 3)]
1 [(1, 0), (1, 1), (1, 2), (1, 3)]
2 [(2, 0), (2, 1), (2, 2), (2, 3)]
3 [(3, 0), (3, 1), (3, 2), (3, 3)]
destination data
0 [(0, 0), (1, 0), (2, 0), (3, 0)]
1 [(0, 1), (1, 1), (2, 1), (3, 1)]
2 [(0, 2), (1, 2), (2, 2), (3, 2)]
3 [(0, 3), (1, 3), (2, 3), (3, 3)]
</pre></div>
</div>
</div></blockquote>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.py_allgather">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">py_allgather</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.py_allgather" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">destlist</span> <span class="pre">=</span> <span class="pre">pc.py_allgather(srcitem)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Each rank sends its srcitem to all other ranks. All ranks assemble the
arriving objects into an nhost size list such that the i’th element
came from the i’th rank.
The destlist is the same on every rank.
The srcitem may be any pickleable Python object including None, Bool, int, h.Vector,
etc. and will appear in the destination list as that type. This method can
only be called from the python interpreter and cannot be called from HOC.
All ranks (or all ranks in a subworld) must participate in this MPI collective.</p>
<p>pc.py_allgather uses less memory and is faster than the equivalent
<code class="docutils literal notranslate"><span class="pre">destlist</span> <span class="pre">=</span> <span class="pre">pc.py_alltoall([srcitem]*nhost)</span></code></p>
</dd>
</dl>
<p>Example:</p>
<blockquote>
<div><blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">nhost</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>

<span class="n">src</span> <span class="o">=</span> <span class="n">rank</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_allgather</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pr</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
  <span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># try to avoid mixing different pr output</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;allgather src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;allgather dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">src</span> <span class="o">=</span> <span class="p">[</span><span class="n">src</span><span class="p">]</span><span class="o">*</span><span class="n">nhost</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_alltoall</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 nrniv -python -mpi test.py
numprocs=4
NEURON -- VERSION 7.6.4-4-gcd480afb master (cd480afb) 2019-01-04
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2018
See http://neuron.yale.edu/neuron/credits

0: allgather src: 0
1: allgather src: 1
2: allgather src: 2
3: allgather src: 3
0: allgather dest: [0, 1, 2, 3]
1: allgather dest: [0, 1, 2, 3]
2: allgather dest: [0, 1, 2, 3]
3: allgather dest: [0, 1, 2, 3]
2: alltoall src: [2, 2, 2, 2]
0: alltoall src: [0, 0, 0, 0]
1: alltoall src: [1, 1, 1, 1]
3: alltoall src: [3, 3, 3, 3]
1: alltoall dest: [0, 1, 2, 3]
2: alltoall dest: [0, 1, 2, 3]
0: alltoall dest: [0, 1, 2, 3]
3: alltoall dest: [0, 1, 2, 3]
</pre></div>
</div>
</div></blockquote>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.py_gather">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">py_gather</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.py_gather" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">destlist_on_root</span> <span class="pre">=</span> <span class="pre">pc.py_gather(srcitem,</span> <span class="pre">root)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Each rank sends its srcitem to the root rank. The root rank assembles the
arriving objects into an nhost size list such that the i’th element came from
the i’th rank.
The destlist_on_root return value for non-root ranks is None.
The srcitem may be any pickleable Python object including None, Bool, int, h.Vector,
etc. and will appear in the destination list as that type. This method can
only be called from the python interpreter and cannot be called from HOC.
All ranks (or all ranks in a subworld) must participate in this MPI collective.</p>
<p>pc.py_gather uses less memory and is faster than the almost equivalent
<code class="docutils literal notranslate"><span class="pre">destlist</span> <span class="pre">=</span> <span class="pre">pc.py_alltoall([srcitem</span> <span class="pre">if</span> <span class="pre">i</span> <span class="pre">==</span> <span class="pre">root</span> <span class="pre">else</span> <span class="pre">None</span> <span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(nhost)])</span></code>
“Almost” because the return value on non-root ranks is None for pc.py_allgather but
a list of nhost None for pc.py_alltoall</p>
</dd>
</dl>
<p>Example:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">nhost</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>

<span class="n">root</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># any specific rank</span>
<span class="n">src</span> <span class="o">=</span> <span class="n">rank</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_gather</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">root</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pr</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
  <span class="n">sleep</span><span class="p">(</span><span class="mf">.1</span><span class="p">)</span> <span class="c1"># try to avoid mixing different pr output</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;gather src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;gather dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">src</span> <span class="o">=</span> <span class="p">[</span><span class="n">src</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">root</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nhost</span><span class="p">)]</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_alltoall</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 nrniv -python -mpi test.py
numprocs=4
NEURON -- VERSION 7.6.4-4-gcd480afb master (cd480afb) 2019-01-04
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2018
See http://neuron.yale.edu/neuron/credits

3: gather src: 3
1: gather src: 1
2: gather src: 2
0: gather src: 0
3: gather dest: None
1: gather dest: None
2: gather dest: None
0: gather dest: [0, 1, 2, 3]
1: alltoall src: [1, None, None, None]
2: alltoall src: [2, None, None, None]
3: alltoall src: [3, None, None, None]
0: alltoall src: [0, None, None, None]
3: alltoall dest: [None, None, None, None]
1: alltoall dest: [None, None, None, None]
2: alltoall dest: [None, None, None, None]
0: alltoall dest: [0, 1, 2, 3]
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prior to NEURON 7.6, pc.nhost() and pc.id() returned a float instead of an int.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.py_scatter">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">py_scatter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.py_scatter" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">destitem_from_root</span> <span class="pre">=</span> <span class="pre">pc.py_scatter(srclist,</span> <span class="pre">root)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The root rank sends the i’th element in its nhost size list to the i’th rank.
The srclist must contain nhost pickleable Python objects including None, Bool,
int, h.Vector,
etc. and will appear in the destination list as that type. This method can
only be called from the python interpreter and cannot be called from HOC.
All ranks (or all ranks in a subworld) must participate in this MPI collective.</p>
<p>pc.py_scatter uses less memory and is faster than the almost equivalent
<code class="docutils literal notranslate"><span class="pre">destitem</span> <span class="pre">=</span> <span class="pre">pc.pyalltoall(srclist</span> <span class="pre">if</span> <span class="pre">rank</span> <span class="pre">==</span> <span class="pre">root</span> <span class="pre">else</span> <span class="pre">[None]*nhost)</span></code>
“Almost” because the return value on rank i for py.pyalltoall is a list
filled with None
except for the root’th item which is the i’th element of srclist of the root rank.</p>
</dd>
</dl>
<p>Example:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">nhost</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>

<span class="n">root</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># any specific rank</span>
<span class="n">src</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nhost</span><span class="p">)]</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">root</span> <span class="k">else</span> <span class="kc">None</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_scatter</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">root</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pr</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
  <span class="n">sleep</span><span class="p">(</span><span class="mf">.1</span><span class="p">)</span> <span class="c1"># try to avoid mixing different pr output</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;scatter src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;scatter dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">src</span> <span class="o">=</span> <span class="n">src</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">root</span> <span class="k">else</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">nhost</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_alltoall</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 nrniv -python -mpi test.py
numprocs=4
NEURON -- VERSION 7.6.4-4-gcd480afb master (cd480afb) 2019-01-04
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2018
See http://neuron.yale.edu/neuron/credits

0: scatter src: [0, 1, 2, 3]
2: scatter src: None
1: scatter src: None
3: scatter src: None
2: scatter dest: 2
0: scatter dest: 0
1: scatter dest: 1
3: scatter dest: 3
1: alltoall src: [None, None, None, None]
2: alltoall src: [None, None, None, None]
0: alltoall src: [0, 1, 2, 3]
3: alltoall src: [None, None, None, None]
0: alltoall dest: [0, None, None, None]
1: alltoall dest: [1, None, None, None]
2: alltoall dest: [2, None, None, None]
3: alltoall dest: [3, None, None, None]
</pre></div>
</div>
</div></blockquote>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.py_broadcast">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">py_broadcast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.py_broadcast" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">destitem_from_root</span> <span class="pre">=</span> <span class="pre">pc.py_broadcast(srcitem,</span> <span class="pre">root)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The root rank sends the srcitem to every rank.
The srcitem can be any pickleable Python object including None, Bool,
int, h.Vector,
etc. and will be returned as that type. This method can
only be called from the python interpreter and cannot be called from HOC.
All ranks (or all ranks in a subworld) must participate in this MPI collective.</p>
<p>pc.py_broadcast uses less memory and is faster than the almost equivalent
<code class="docutils literal notranslate"><span class="pre">destitem</span> <span class="pre">=</span> <span class="pre">pc.pyalltoall([srcitem]*nhost</span> <span class="pre">if</span> <span class="pre">rank</span> <span class="pre">==</span> <span class="pre">root</span> <span class="pre">else</span> <span class="pre">[None]*nhost)</span></code>
“Almost” because the return value on rank i for py.pyalltoall is a list
filled with None
except for the root’th item which is a copy of srcitem from the root rank.</p>
</dd>
</dl>
<p>Example:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">nhost</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>

<span class="n">root</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># any specific rank</span>
<span class="n">src</span> <span class="o">=</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">root</span> <span class="k">else</span> <span class="kc">None</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_broadcast</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">root</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pr</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
  <span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
  <span class="n">sleep</span><span class="p">(</span><span class="mf">.1</span><span class="p">)</span> <span class="c1"># try to avoid mixing different pr output</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;broadcast src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;broadcast dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">src</span> <span class="o">=</span> <span class="p">[</span><span class="n">src</span><span class="p">]</span><span class="o">*</span><span class="n">nhost</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">root</span> <span class="k">else</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">nhost</span>
<span class="n">dest</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">py_alltoall</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
<span class="n">pr</span><span class="p">(</span><span class="s2">&quot;alltoall dest&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="p">)</span>

<span class="n">pc</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="n">h</span><span class="o">.</span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ mpiexec -n 4 nrniv -python -mpi test.py
numprocs=4
NEURON -- VERSION 7.6.4-4-gcd480afb master (cd480afb) 2019-01-04
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2018
See http://neuron.yale.edu/neuron/credits

0: broadcast src: 0
1: broadcast src: None
2: broadcast src: None
3: broadcast src: None
0: broadcast dest: 0
1: broadcast dest: 0
2: broadcast dest: 0
3: broadcast dest: 0
2: alltoall src: [None, None, None, None]
0: alltoall src: [0, 0, 0, 0]
1: alltoall src: [None, None, None, None]
3: alltoall src: [None, None, None, None]
1: alltoall dest: [0, None, None, None]
2: alltoall dest: [0, None, None, None]
0: alltoall dest: [0, None, None, None]
3: alltoall dest: [0, None, None, None]
</pre></div>
</div>
</div></blockquote>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.broadcast">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">broadcast</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.broadcast" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.broadcast(strdef,</span> <span class="pre">root)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.broadcast(vector,</span> <span class="pre">root)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Every host gets the value from the host with pc.id == root.
The vector is resized to the size of the root host vector.
The return value is the length of the string or the size of the vector.
At the time that each other-than-root host reaches this statement
they receive the values sent from the root host.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
<section id="subworld">
<span id="id1"></span><h2>SubWorld<a class="headerlink" href="#subworld" title="Link to this heading"></a></h2>
<dl>
<dt>Description:</dt><dd><p>Without the methods discussed in this section,
the bulletin board and parallel network styles cannot be used together.
The parallel network style relies heavily on synchronization through
the use of blocking collective communication
methods and load balance is the primary consideration. The bulletin board
style is assynchronous and a process works on a submitted task generally
without communicating with other tasks except possibly and indirectly through
posting and taking messages on the bulletin board.
Without the subworld method, at most the network style can be used and then
switched to bulletin board style. The only way to simulate a parallel
network after executing <a class="reference internal" href="#ParallelContext.runworker" title="ParallelContext.runworker"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.runworker()</span></code></a> would be to utilize
the <a class="reference internal" href="#ParallelContext.context" title="ParallelContext.context"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.context()</span></code></a> method. In particular, without subworlds,
it is impossible to correctly submit bulletin board tasks, each of which
simulates a network specfied with the <a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a>
methods — even if the network is complete on a single process. This is
because network simulations internally call MPI collectives which would
deadlock due to not every worker participating in the collective.</p>
<p>The <a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.subworlds()</span></code></a> method divides the world of processors into subworlds,
each of which can execute a task (in parallel within the subworld) that independently and assynchronously
creates and simulates (and destroys if the task networks are different)
a separate
network described using the <a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a> and
<a class="reference internal" href="#paralleltransfer"><span class="std std-ref">Parallel Transfer</span></a> methods. The task, executing
in the subworld can also make use of the <a class="reference internal" href="#parallelcontext-mpi"><span class="std std-ref">MPI</span></a> collectives.
Different subworlds can use the same global identifiers without
interference and the spike communication, transfers, and MPI collectives
are localized to within a subworld. I.e. in MPI terms,
each subworld utilizes a distinct MPI communicator. In a subworld, the
<a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> and <a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a> refer to the rank and
number of processors in the subworld. (Note that every subworld has
a <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> == 0 rank processor.)</p>
<p>Only the rank <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> == 0 subworld processors communicate
with the bulletin board. Of these processors, one
(<a class="reference internal" href="#ParallelContext.id_world" title="ParallelContext.id_world"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_world()</span></code></a> == 0, i.e. <a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> == 0 and <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id()</span></code></a> == 0)
is the master processor and the others
(<a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> &gt; 0 and <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id()</span></code></a> == 0)
are the workers. The master
submits tasks to the bulletin board (and executes the task in parallel as a subworld with its total nhost ranks if no results
are available) and the workers execute tasks and id == 0 processes post the results
to the bulletin board. Remember, all the workers also have <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a>
== 0 but different <a class="reference internal" href="#ParallelContext.id_world" title="ParallelContext.id_world"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_world()</span></code></a> and <a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> ranks. The subworld
<a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> ranks greater than 0 are not workers in the sense of directly interacting with the bulletin board by posting and taking messages — their
global rank is <a class="reference internal" href="#ParallelContext.id_world" title="ParallelContext.id_world"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_world()</span></code></a> but their bulletin board rank, <a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> is -1. To reduce confusion, it would be nice if the id_bbs value was the same as that of rank 0 of the subworld. But see the following paragraph for why the -1 value has the edge in usefulness. Fortunately the subworld (id_bbs) identifier is for these id()&gt;0 ranks is easy to calculate as <code class="docutils literal notranslate"><span class="pre">pc.id_world()//requested_subworld_size</span></code> where the denominator is the argument used in <a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.subworlds()</span></code></a>.
(One cannot use <code class="docutils literal notranslate"><span class="pre">pc.nhost()</span></code> in place of the <code class="docutils literal notranslate"><span class="pre">requested_subworld_size</span></code> arg if <code class="docutils literal notranslate"><span class="pre">pc.nhost_world()</span></code> is not an integer multiple of <code class="docutils literal notranslate"><span class="pre">`requested_subworld_size</span></code> since the last subworld will have <code class="docutils literal notranslate"><span class="pre">pc.nhost()</span> <span class="pre">&lt;</span> <span class="pre">requested_subworld_size</span></code>.)
When a worker (or the master) receives a task to execute, the exact same
function with arguments that define the task will be executed on all the
processes of the subworld. A subworld is exactly analogous to the old
world of a network simulation in which processes distinguish themselves
by means of <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> which is unique among
the <a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a> processes in the subworld.</p>
<p>A runtime error will result if an <a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> == -1 rank processor tries
to communicate with the bulletin board, thus the general idiom for
a task posting or taking information from the bulletin board should be
<code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">(pc.id</span> <span class="pre">==</span> <span class="pre">0)</span> <span class="pre">{</span> <span class="pre">...</span> <span class="pre">}</span></code> or if (pc.id_bbs != -1) { … }.
The latter is more general since the former would not be correct if
<a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-meth docutils literal notranslate"><span class="pre">subworlds()</span></code></a> has NOT been called since in that case
<code class="docutils literal notranslate"><span class="pre">pc.id</span> <span class="pre">==</span> <span class="pre">pc.id_world</span> <span class="pre">==</span> <span class="pre">pc.id_bbs</span></code> and
<code class="docutils literal notranslate"><span class="pre">pc.nhost</span> <span class="pre">==</span> <span class="pre">pc.nhost_world</span> <span class="pre">==</span> <span class="pre">pc.nhost_bbs</span></code></p>
</dd>
</dl>
<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.subworlds">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">subworlds</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.subworlds" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.subworlds(subworld_size)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Divides the world of all processors
into <a class="reference internal" href="#ParallelContext.nhost_world" title="ParallelContext.nhost_world"><code class="xref py py-func docutils literal notranslate"><span class="pre">nhost_world()</span></code></a> / subworld_size subworlds.
Note that the total number of processes, nhost_world, should be
an integer multiple of subworld_size. If that is not the case, the
last subworld will have <code class="docutils literal notranslate"><span class="pre">pc.nhost()</span> <span class="pre">&lt;</span> <span class="pre">subworld_size</span></code>.
The most useful subworld sizes are 1 and <a class="reference internal" href="#ParallelContext.nhost_world" title="ParallelContext.nhost_world"><code class="xref py py-func docutils literal notranslate"><span class="pre">nhost_world()</span></code></a> .
After return, for the processes
in each subworld, <a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a> is equal to subworld_size (except the last if nhost_world is not in integer multiple of subworld_size).
and the <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> is the rank of the process with respect
to the subworld of which it is a part.</p>
<p>Each subworld has its own
unique MPI communicator for the <a class="reference internal" href="#parallelcontext-mpi"><span class="std std-ref">MPI</span></a> functions such
as <a class="reference internal" href="#ParallelContext.barrier" title="ParallelContext.barrier"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.barrier()</span></code></a> and so those collectives do not affect other subworlds.
All the <a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a> notions are local to a subworld. I.e. independent
networks using the same gids can be simulated simultaneously in
different subworlds. Only rank 0 of a subworld ( <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a>
== 0) can use the bulletin board and has a non-negative <a class="reference internal" href="#ParallelContext.nhost_bbs" title="ParallelContext.nhost_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nhost_bbs()</span></code></a>
and <a class="reference internal" href="#ParallelContext.id_bbs" title="ParallelContext.id_bbs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_bbs()</span></code></a> .</p>
<p>Thus the bulletin board interacts with <a class="reference internal" href="#ParallelContext.nhost_bbs" title="ParallelContext.nhost_bbs"><code class="xref py py-func docutils literal notranslate"><span class="pre">nhost_bbs()</span></code></a> processes
each with <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> == 0. And each of those rank 0 processes
interacts with <a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a> processes using MPI commands
isolated within each subworld.</p>
<p>Probably the most useful values of subworld_size are 1 and <a class="reference internal" href="#ParallelContext.nhost_world" title="ParallelContext.nhost_world"><code class="xref py py-func docutils literal notranslate"><span class="pre">nhost_world()</span></code></a>.
The former uses the bulletin board to communicate between all processes
but allows the use of gid specified networks within each process. ie.
one master and nhost_world - 1 workers.
The latter uses all processes to simulate a parallel network and there
is only one process, the master,
(<a class="reference internal" href="#ParallelContext.id_world" title="ParallelContext.id_world"><code class="xref py py-meth docutils literal notranslate"><span class="pre">id_world()</span></code></a> == 0) interacting with the bulletin board.</p>
</dd>
<dt>Example:</dt><dd><p>The following example is intended to be run with 6 processes. The subworlds
function with an argument of 3 will divide the 6 process world into
two subworlds each with 3 processes. To aid in seeing how the computation
progresses the function “f” prints its rank and number of processors
for the world, bulletin board, and net (subworld) as well as argument,
return value, and bulletin board defined userid. Prior to the runworker
call, all processes call f. After the runworker call, only the master
process returns and calls f. The master submits 4 tasks and then enters
a while loop waiting for results and, when a result is ready, prints
the userid, argument, and return value of the task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span>
<span class="n">h</span><span class="o">.</span><span class="n">nrnmpi_init</span><span class="p">()</span> <span class="c1">#does nothing if mpi4py succeeded</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">pc</span><span class="o">.</span><span class="n">subworlds</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">id_world</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">+</span> <span class="n">pc</span><span class="o">.</span><span class="n">id_bbs</span><span class="p">()</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;userid=</span><span class="si">%d</span><span class="s2"> arg=</span><span class="si">%d</span><span class="s2"> ret=</span><span class="si">%03d</span><span class="s2">  world </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">  bbs </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">  net </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
        <span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">hoc_ac_</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">pc</span><span class="o">.</span><span class="n">id_world</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost_world</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">id_bbs</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost_bbs</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">id</span><span class="p">(),</span> <span class="n">pc</span><span class="o">.</span><span class="n">nhost</span><span class="p">()))</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>

<span class="n">h</span><span class="o">.</span><span class="n">hoc_ac_</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">if</span> <span class="p">(</span><span class="n">pc</span><span class="o">.</span><span class="n">id_world</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;before runworker&quot;</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pc</span><span class="o">.</span><span class="n">runworker</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">after runworker&quot;</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">before submit&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">pc</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;after submit&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">userid</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">working</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">userid</span><span class="p">:</span> <span class="k">break</span>
    <span class="n">arg</span> <span class="o">=</span> <span class="n">pc</span><span class="o">.</span><span class="n">upkscalar</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;result userid=</span><span class="si">%d</span><span class="s2"> arg=</span><span class="si">%d</span><span class="s2"> return=</span><span class="si">%03d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">userid</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="n">pc</span><span class="o">.</span><span class="n">pyret</span><span class="p">()))</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">after working&quot;</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="n">pc</span><span class="o">.</span><span class="n">done</span><span class="p">()</span>
</pre></div>
</div>
<p>If the above code is saved in <code class="file docutils literal notranslate"><span class="pre">temp.py</span></code> and executed with 6 processes using
<code class="docutils literal notranslate"><span class="pre">mpiexec</span> <span class="pre">-n</span> <span class="pre">6</span> <span class="pre">nrniv</span> <span class="pre">-mpi</span> <span class="pre">temp.py</span></code> then the output will look like
(some lines may be out of order)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$  mpirun -n 6 python temp.py
numprocs=6
NEURON -- VERSION 7.5 master (266b5a0) 2017-05-22
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2016
See http://neuron.yale.edu/neuron/credits

before runworker
userid=-1 arg=1 ret=000  world 0 of 6  bbs 0 of 2  net 0 of 3
userid=-1 arg=1 ret=091  world 1 of 6  bbs -1 of -1  net 1 of 3
userid=-1 arg=1 ret=492  world 5 of 6  bbs -1 of -1  net 2 of 3
userid=-1 arg=1 ret=192  world 2 of 6  bbs -1 of -1  net 2 of 3
userid=-1 arg=1 ret=310  world 3 of 6  bbs 1 of 2  net 0 of 3
userid=-1 arg=1 ret=391  world 4 of 6  bbs -1 of -1  net 1 of 3

after runworker
userid=-1 arg=2 ret=000  world 0 of 6  bbs 0 of 2  net 0 of 3

before submit
after submit
userid=21 arg=4 ret=000  world 0 of 6  bbs 0 of 2  net 0 of 3
userid=21 arg=4 ret=091  world 1 of 6  bbs -1 of -1  net 1 of 3
userid=20 arg=3 ret=391  world 4 of 6  bbs -1 of -1  net 1 of 3
userid=20 arg=3 ret=492  world 5 of 6  bbs -1 of -1  net 2 of 3
userid=21 arg=4 ret=192  world 2 of 6  bbs -1 of -1  net 2 of 3
userid=20 arg=3 ret=310  world 3 of 6  bbs 1 of 2  net 0 of 3
result userid=21 arg=4 return=000
result userid=20 arg=3 return=310
userid=23 arg=6 ret=000  world 0 of 6  bbs 0 of 2  net 0 of 3
userid=23 arg=6 ret=091  world 1 of 6  bbs -1 of -1  net 1 of 3
userid=22 arg=5 ret=391  world 4 of 6  bbs -1 of -1  net 1 of 3
userid=22 arg=5 ret=492  world 5 of 6  bbs -1 of -1  net 2 of 3
userid=23 arg=6 ret=192  world 2 of 6  bbs -1 of -1  net 2 of 3
userid=22 arg=5 ret=310  world 3 of 6  bbs 1 of 2  net 0 of 3
result userid=23 arg=6 return=000
result userid=22 arg=5 return=310

after working
userid=0 arg=7 ret=000  world 0 of 6  bbs 0 of 2  net 0 of 3
$
</pre></div>
</div>
<p>One can see from the output that before the runworker call, all the
processes called f. After runworker, only the master returned so there
is only one call to f. All tasks were submitted to the bulletin
board before any task generated print output. In this case, during
the while loop, the master started on the task with arg=4 and the two
associates within that subworld also executed f(4). Only the master
returned the result of f(4) to the bulletin board (the return values
of the two subworld associates were discarded). The master and its network
associates also executed f(5) and f(6). f(3) was executed by the world
rank 3 process (bbs rank 1, net rank 0) and that subworlds two net associates.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nhost_world">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nhost_world</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nhost_world" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">numprocs</span> <span class="pre">=</span> <span class="pre">pc.nhost_world()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Total number of processes in all subworlds. Equivalent to
<a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a> when <a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-func docutils literal notranslate"><span class="pre">subworlds()</span></code></a> has not been executed.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.id_world">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">id_world</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.id_world" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">rank</span> <span class="pre">=</span> <span class="pre">pc.id_world()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Global world rank of the process. This is unique among all processes
of all subworlds and ranges from 0 to <a class="reference internal" href="#ParallelContext.nhost_world" title="ParallelContext.nhost_world"><code class="xref py py-func docutils literal notranslate"><span class="pre">nhost_world()</span></code></a> - 1</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nhost_bbs">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nhost_bbs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nhost_bbs" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">numprocs</span> <span class="pre">=</span> <span class="pre">pc.nhost_bbs()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>If <a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-func docutils literal notranslate"><span class="pre">subworlds()</span></code></a> has been called, nhost_bbs() returns the number of
subworlds if <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> == 0 and -1 for all other ranks in
the subworld.
If subworlds has NOT been called then nhost_bbs, nhost_world, and nhost
are the same.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.id_bbs">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">id_bbs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.id_bbs" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">rank</span> <span class="pre">=</span> <span class="pre">pc.id_bbs()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>If <a class="reference internal" href="#ParallelContext.subworlds" title="ParallelContext.subworlds"><code class="xref py py-func docutils literal notranslate"><span class="pre">subworlds()</span></code></a> has been called id_bbs() returns the subworld rank
if <a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a> == 0 and -1 for all other ranks in the
subworld.
If subworlds has not been called then id_bbs, id_world, and id are the
same.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
<section id="parallel-network">
<span id="parallelnetwork"></span><h2>Parallel Network<a class="headerlink" href="#parallel-network" title="Link to this heading"></a></h2>
<dl>
<dt>Description:</dt><dd><p>Extra methods for the ParallelContext that pertain to parallel network
simulations where cell communication involves discrete logical spike events.</p>
<p>The methods described in this section work for intra-machine connections
regardless of how NEURON is configured (Thus all parallel network models can
be executed on any serial machine). However machine spanning
connections can only be made if NEURON has been configured with
the –with-mpi option (or other options that automatically set it such as
–with-paranrn). (See <a class="reference internal" href="#parallelcontext-mpi"><span class="std std-ref">MPI</span></a> for installation hints).</p>
<p>The fundamental requirement is that each
cell be associated with a unique integer global id (gid). The
<a class="reference internal" href="parnet.html#ParallelNetManager" title="ParallelNetManager"><code class="xref py py-func docutils literal notranslate"><span class="pre">ParallelNetManager()</span></code></a> in nrn/share/lib/hoc/netparmpi.hoc is a sample
implementation that makes use of these facilities. That implementation
assumes that all conductance based cells contain a public
<code class="docutils literal notranslate"><span class="pre">connect2target(targetsynapse,</span> <span class="pre">netcon)</span></code> which connects the target synapse
object to a specific range variable (e.g. soma.v(.5)) and returns the
new NetCon in the second object argument. Artificial cells may either be
bare or wrapped in class and made public as a Point Process object field. That is,
cells built as NetworkReadyCells are compatible with the
ParallelNetManager and that manager follows as closely as possible
the style of network construction used by the NetGUI builder.</p>
<p>Notes:</p>
<p>Gid, sid, and pieces.</p>
<p>The typical network simulation sets up
a one to one correspondence between gid and cell.
This most common usage is suggested by
the method name, <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a>, that makes the correspondence
as well as the accessor method, <a class="reference internal" href="#ParallelContext.gid2cell" title="ParallelContext.gid2cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid2cell()</span></code></a>.
That’s because,
almost always, a cell has one spike detection site and the entire cell is
on a single cpu. But either or both of those assertions can break down
and then one must be aware that, rigorously,
a gid is associated with a spike detection site (defined by
a NetCon source). For example,
many spike detection sites per cell are useful for reciprocal synapses.
Each side of each reciprocal synapse will require its own distinct gid.
When load balance is a problem, or when you have more cpus than cells,
it is useful to split cells into pieces and put the pieces on different
cpus (<a class="reference internal" href="#ParallelContext.splitcell" title="ParallelContext.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.splitcell()</span></code></a> and <a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a>).
But now, some pieces will not have a spike detection site and therefore
don’t have to have a gid. In either case, it can be administratively
useful to invent an administrative policy for gid values that encodes
whole cell identification. For a cell piece that has no spike output,
one can still give it a gid associated with an arbitrary spike detection
site that is effectively turned off because it is not the source for
any existing NetCon and it was never specified as an
<a class="reference internal" href="#ParallelContext.outputcell" title="ParallelContext.outputcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.outputcell()</span></code></a>. In the same way, it is also
useful to encode a <a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a>
sid (split id) with whole cell identification.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If mpi is
not available but NEURON has been built with PVM installed, an alternative
ParallelNetManager implementation with the identical interface is
available that makes use only of standard ParallelContext methods.</p>
</div>
<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.set_gid2node">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">set_gid2node</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.set_gid2node" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.set_gid2node(gid,</span> <span class="pre">id)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>If the id is equal to pc.id then this machine “owns” the gid and
the associated cell
should be eventually created only on this machine.
Note that id must be in the range 0 to pc.nhost()-1. The global id (gid)
can be any unique integer &gt;= 0 but generally ranges from 0 to ncell-1 where
ncell is the total number of real and artificial cells.</p>
<p>Commonly, a cell has only one spike detector location and hence we normally
identify a gid with a cell. However,
cell can have several distinct spike detection locations or spike
detector point processes and each must be
associated with a distinct gid. (e.g. dendro-dendritic synapses).</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.id" title="ParallelContext.id"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.id()</span></code></a>, <a class="reference internal" href="#ParallelContext.nhost" title="ParallelContext.nhost"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.nhost()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.gid_exists">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">gid_exists</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.gid_exists" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">integer</span> <span class="pre">=</span> <span class="pre">pc.gid_exists(gid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return 3 if the gid is owned by this machine and the gid is already
associated with an output cell in the sense that its spikes will be
sent to all other machines. (i.e. <a class="reference internal" href="#ParallelContext.outputcell" title="ParallelContext.outputcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.outputcell()</span></code></a> has
also been called with that gid or <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a> has been
called with a third arg of 1.)</p>
<p>Return 2 if the gid is owned by this machine and has been associated with
a NetCon source location via the <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-func docutils literal notranslate"><span class="pre">cell()</span></code></a> method.</p>
<p>Return 1 if the gid is owned by this machine but has not been associated with
a NetCon source location.</p>
<p>Return 0 if the gid is NOT owned by this machine.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.threshold">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">threshold</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.threshold" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">th</span> <span class="pre">=</span> <span class="pre">pc.threshold(gid)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">th</span> <span class="pre">=</span> <span class="pre">pc.threshold(gid,</span> <span class="pre">th)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return the threshold of the source variable determined by the first arg
of the <a class="reference internal" href="netcon.html#NetCon" title="NetCon"><code class="xref py py-func docutils literal notranslate"><span class="pre">NetCon()</span></code></a> constructor which is used to associate the gid with a
source variable via <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-func docutils literal notranslate"><span class="pre">cell()</span></code></a> . If the second arg is present the threshold
detector is given that threshold. This method can only be called if the
gid is owned by this machine and <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-func docutils literal notranslate"><span class="pre">cell()</span></code></a> has been previously called.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.cell">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">cell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.cell" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.cell(gid,</span> <span class="pre">netcon)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.cell(gid,</span> <span class="pre">netcon,</span> <span class="pre">0)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The cell which is the source of the <a class="reference internal" href="netcon.html#NetCon" title="NetCon"><code class="xref py py-func docutils literal notranslate"><span class="pre">NetCon()</span></code></a> is associated with the global
id. By default,(no third arg or third arg = 1)
the spikes generated by that cell will be sent to every other machine
(see <a class="reference internal" href="#ParallelContext.outputcell" title="ParallelContext.outputcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.outputcell()</span></code></a>). A cell commonly has only one spike
generation location, but, for example in the case of reciprocal
dendro-dendritic synapses, there is no reason why it cannot have several.
The NetCon source defines the spike generation location.
Note that it is an error if the gid does not exist on this machine. The
normal idiom is to use a NetCon returned by a call to the cell’s
connect2target(None, netcon) method or else, if the cell is an unwrapped
artificial cell, use a <code class="docutils literal notranslate"><span class="pre">netcon</span> <span class="pre">=</span> <span class="pre">h.NetCon(cell,</span> <span class="pre">None)</span></code> statement.
In either case, after
ParallelContext.cell() has been called, this NetCon can be
destroyed to save memory; the spike detection threshold
can be accessed by <a class="reference internal" href="#ParallelContext.threshold" title="ParallelContext.threshold"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.threshold()</span></code></a>, and the
weights and delays of projections to synaptic targets
are specified by the parameters of the NetCons created
by <a class="reference internal" href="#ParallelContext.gid_connect" title="ParallelContext.gid_connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid_connect()</span></code></a>.</p>
<p>Note that cells which do not send spikes to other machines are not required
to call this and in fact do not need a gid. However the administrative
detail would be significantly more complicated due to the multiplication
of cases in regard to whether the source and target exist AND the source
is an outputcell.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.outputcell">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">outputcell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.outputcell" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.outputcell(gid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Spikes this cell generates are to be distributed to all the other machines.
Note that <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a> needs to be called prior to this and this
does not need to be called if the third arg of that was non-zero.
In principle there is no reason for a cell to even have a gid if it is not
an outputcell. However the separation between pc.cell and pc.outputcell
allows uniform administrative setup of the network to defer marking a cell
as an output cell until an actual machine spanning connection is made for
which the source is on this machine and the target is on another machine.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.spike_record">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">spike_record</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.spike_record" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.spike_record(gid,</span> <span class="pre">spiketimevector,</span> <span class="pre">gidvector)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>This is a synonym for <a class="reference internal" href="netcon.html#NetCon.record" title="NetCon.record"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NetCon.record()</span></code></a> but obviates the requirement of
creating a NetCon using information about the source cell that is
relatively more tedious to obtain. This can only be called on the source
cell’s machine. Note that a prerequisite is a call
to <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a> . A call to <a class="reference internal" href="#ParallelContext.outputcell" title="ParallelContext.outputcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.outputcell()</span></code></a> is NOT
a prerequisite.</p>
<p>If the gid arg is -1, then spikes from ALL output gids on this
machine will be recorded.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.gid_connect">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">gid_connect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.gid_connect" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">netcon</span> <span class="pre">=</span> <span class="pre">pc.gid_connect(srcgid,</span> <span class="pre">target)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">netcon</span> <span class="pre">=</span> <span class="pre">pc.gid_connect(srcgid,</span> <span class="pre">target,</span> <span class="pre">netcon)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>A virtual connection is made between the source cell global id (which
may or may not
be owned by this machine) and the target (a synapse or artificial cell object)
which EXISTS on this machine. A <a class="reference internal" href="netcon.html#NetCon" title="NetCon"><code class="xref py py-class docutils literal notranslate"><span class="pre">NetCon</span></code></a> object is returned and the
full delay for the connection should be given to it (as well as the weight).
This is not the NetCon that
monitors the spike source variable for threshold crossings, so its
threshold parameter will not affect simulations.  Threshold crossings
are determined by the detector that belonged to the NetCon used to
associate the presynaptic spike source with a gid (see
<a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a> and <a class="reference internal" href="#ParallelContext.threshold" title="ParallelContext.threshold"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.threshold()</span></code></a>).</p>
<p>Note that if the srcgid is owned by this machine then <a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-func docutils literal notranslate"><span class="pre">cell()</span></code></a> must be called
earlier to make sure that the srcgid is associated with a NetCon source
location.</p>
<p>Note that if the srcgid is not owned by this machine, then this machines
target will only get spikes from the srcgid if the source gid’s machine
had called <a class="reference internal" href="#ParallelContext.outputcell" title="ParallelContext.outputcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.outputcell()</span></code></a> or the third arg of
<a class="reference internal" href="#ParallelContext.cell" title="ParallelContext.cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.cell()</span></code></a> was 1.</p>
<p>If the third arg exists, it must be a NetCon object with target the same
as the second arg. The src of that NetCon will be replaced by srcgid and
that NetCon returned. The purpose is to re-establish a connection to
the original srcgid after a <a class="reference internal" href="#ParallelContext.gid_clear" title="ParallelContext.gid_clear"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid_clear()</span></code></a> .</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.psolve">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">psolve</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.psolve" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.psolve(tstop)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>This should be called on every machine to start something analogous to
cvode.solve(tstop). In fact, if the variable step method is invoked this
is exactly what will end up happening except the solve will be broken into
steps determined by the result of <a class="reference internal" href="#ParallelContext.set_maxstep" title="ParallelContext.set_maxstep"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.set_maxstep()</span></code></a>.</p>
</dd>
<dt>Note:</dt><dd><p>If CoreNEURON is active, psolve will be executed in CoreNEURON.
Calls to psolve with CoreNEURON active and inactive can be
interleaved and calls to finitialize can be interspersed. The result
should be exactly the same as if all execution was done in NEURON
(except for round-off error differences due to high performance
optimizations).</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.timeout">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">timeout</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.timeout" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">oldtimeout</span> <span class="pre">=</span> <span class="pre">pc.timeout(seconds)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>During execution of <a class="reference internal" href="#ParallelContext.psolve" title="ParallelContext.psolve"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.psolve()</span></code></a> ,
sets the timeout for when to abort when seconds pass and t does not
increase.  Returns the old timeout.  The standard timeout is 20 seconds.
If the arg is 0, then there is no timeout.
The purpose of a timeout is to avoid wasting time on massively
parallel supercomputers when an error occurs such that one would wait
forever in a collective.  This function allows one to change the timeout
in those rare cases during a simulation where processes have to wait on
some process to finish a large amount work or some time step has an
extreme load imbalance.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.set_maxstep">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">set_maxstep</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.set_maxstep" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">local_minimum_delay</span> <span class="pre">=</span> <span class="pre">pc.set_maxstep(default_max_step)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>This should be called on every machine after all the NetCon delays have
been specified. It looks at all the delays on all the machines
associated with the netcons
created by the <a class="reference internal" href="#ParallelContext.gid_connect" title="ParallelContext.gid_connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid_connect()</span></code></a> calls, ie the netcons
that conceptually span machines, and sets every machine’s maximum step
size to the minimum delay of those netcons
(but not greater than default_max_step). The method returns this machines
minimum spanning netcon delay.  Assuming computational balance, generally
it is better to maximize the step size since it means fewer MPI_Allgather
collective operations per unit time.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note: No spikes can be delivered between machines unless this method
is called. finitialize relies on this method having been called.
If any trans-machine NetCon delay is reduced below the
step size, this method MUST be called again. Otherwise an INCORRECT
simulation will result.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.spike_compress">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">spike_compress</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.spike_compress" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="samp docutils literal notranslate"><span class="pre">nspike</span> <span class="pre">=</span> <span class="pre">pc.spike_compress(</span><em><span class="pre">nspike</span></em><span class="pre">,</span> <em><span class="pre">gid_compress</span></em><span class="pre">,</span> <em><span class="pre">xchng_meth</span></em><span class="pre">)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>If nspike &gt; 0, selects an alternative implementation of spike exchange
that significantly compresses the buffers and can reduce interprocessor
spike exchange time by a factor of 10. This works only with the
fixed step methods. The optional second argument is 1 by default and
works only if the number of cells on each cpu is less than 256.
Nspike refers to the number of (spiketime, gid) pairs that fit into the
fixed buffer that is exchanged every <a class="reference internal" href="#ParallelContext.set_maxstep" title="ParallelContext.set_maxstep"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_maxstep()</span></code></a> integration interval.
(overflow in the case where more spikes are generated in the interval
than can fit into the first buffer are exchanged when necessary by
a subsequent MPI_Allgatherv collective.) If necessary, the integration
interval is reduced so that there are less than 256 dt steps in the
interval. This allows the default (double spiketime, int gid) which
is at least 12 and possible 16 bytes in size to be reduced to a two
byte sequence.</p>
<p>This method should only be called after the entire network has
been set up since the gid compression mapping requires a knowledge
of which cells are sending interprocessor spikes.</p>
<p>If nspike = 0 , compression is turned off.</p>
<p>If nspike &lt; 0, the current value of nspike is returned.</p>
<p>If gid_compress = 0, or if some cpu has more than 256 cells that send
interprocessor spikes, the real 4 byte integer gids are used in the
(spiketime, gid) pairs and only the spiketime is compressed to 1 byte. i.e.
instead of 2 bytes the pair consists of 5 bytes.</p>
<p>xchng_meth is a bit-field.
bits | usage</p>
<blockquote>
<div><p>0 | 0: Allgather, 1: Multisend (MPI_ISend)
1 | unused
2 | 0: multisend_interval = 1, 1: multisend_interval = 2
3 | 0: don’t use phase2, 1: use phase2</p>
</div></blockquote>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="../../../simctrl/cvode.html#CVode.queue_mode" title="CVode.queue_mode"><code class="xref py py-meth docutils literal notranslate"><span class="pre">CVode.queue_mode()</span></code></a></p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.gid2obj">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">gid2obj</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.gid2obj" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">=</span> <span class="pre">pc.gid2obj(gid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The cell or artificial cell object is returned that is associated with the
global id. Note that the gid must be owned by this machine. If the gid is
associated with a POINT_PROCESS that is located in a section which in turn
is inside an object, this method returns the POINT_PROCESS object.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.gid_exists" title="ParallelContext.gid_exists"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid_exists()</span></code></a>, <a class="reference internal" href="#ParallelContext.gid2cell" title="ParallelContext.gid2cell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid2cell()</span></code></a></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that if a cell has several spike detection sources with different
gids, this is the method to use to return the POINT_PROCESS object itself.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.gid2cell">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">gid2cell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.gid2cell" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">object</span> <span class="pre">=</span> <span class="pre">pc.gid2cell(gid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The cell or artificial cell object is returned that is associated with the
global id. Note that the gid must be owned by this machine.
If the gid is
associated with a POINT_PROCESS that is located in a section which in turn
is inside an object, this method returns the cell object, not the POINT_PROCESS
object.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.gid_exists" title="ParallelContext.gid_exists"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid_exists()</span></code></a>, <a class="reference internal" href="#ParallelContext.gid2obj" title="ParallelContext.gid2obj"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.gid2obj()</span></code></a></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that if a cell has several spike detection sources with different
gids, there is no way to distinguish them with this method. With those gid
arguments, gid2cell would
return the same cell where they are located.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.spike_statistics">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">spike_statistics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.spike_statistics" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">nsendmax</span> <span class="pre">=</span> <span class="pre">pc.spike_statistics(_ref_nsend,</span> <span class="pre">_ref_nrecv,</span> <span class="pre">_ref_nrecv_useful)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns the spanning spike statistics since the last <a class="reference internal" href="../../../simctrl/programmatic.html#finitialize" title="finitialize"><code class="xref py py-func docutils literal notranslate"><span class="pre">finitialize()</span></code></a> . All arguments
are optional.</p>
<p>nsendmax is the maximum number of spikes sent from this machine to all
other machines due to a single maximum step interval.</p>
<p>nsend is the total number of spikes sent from this machine to all other machines.</p>
<p>nrecv is the total number of spikes received by this machine. This
number is the same for all machines.</p>
<p>nrecv_useful is the total number of spikes received from other machines that
are sent to cells on this machine. (note: this does not include any
nsend spikes from this machine)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#ParallelContext.wait_time" title="ParallelContext.wait_time"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.wait_time()</span></code></a>, <a class="reference internal" href="#ParallelContext.set_maxstep" title="ParallelContext.set_maxstep"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.set_maxstep()</span></code></a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The arguments for this function must be NEURON references to numbers; these can be
created via, e.g. <code class="docutils literal notranslate"><span class="pre">_ref_nsend</span> <span class="pre">=</span> <span class="pre">h.ref(0)</span></code> and then dereferenced to get their
values via <code class="docutils literal notranslate"><span class="pre">_ref_nsend[0]</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.max_histogram">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">max_histogram</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.max_histogram" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.max_histogram(vec)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The vector, vec, of size maxspikes, is used to accumulate histogram information about the
maximum number of spikes sent by any cpu during the spike exchange process.
Every spike exchange, vec[max_spikes_sent_by_any_host] is incremented by 1.
It only makes sense to do this on one cpu, normally pc.id() == 0.
If some host sends more than maxspikes at the end of an
integration interval, no element of vec is incremented.</p>
<p>Note that the current implementation of the spike exchange mechanism uses
MPI_Allgather with a fixed buffer size that allows up to nrn_spikebuf_size
spikes per cpu to be sent to all other machines. The default value of this
is 0. If some cpu needs to send more than this number of spikes, then
a second MPI_Allgatherv is used to send the overflow.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
</section>
<section id="parallel-transfer">
<span id="paralleltransfer"></span><h2>Parallel Transfer<a class="headerlink" href="#parallel-transfer" title="Link to this heading"></a></h2>
<blockquote>
<div><dl>
<dt>Description:</dt><dd><p>Extends the <a class="reference internal" href="#parallelcontext-mpi"><span class="std std-ref">MPI</span></a> <a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a> methods to allow parallel simulation
of models involving gap junctions and/or
synapses where the postsynaptic conductance continuously
depends on presynaptic voltage.
Communication overhead for such models
is far greater than when the only communication between cells is with
discrete events. The greater overhead is due to the requirement for
exchanging information every time step.</p>
<p>Gap junctions are assumed to couple cells relatively weakly so that
the modified euler method is acceptable for accuracy and stability.
For purposes of load balance, and regardless of coupling strength,
a cell may be split into two subtrees
with each on a different processor. See <a class="reference internal" href="#ParallelContext.splitcell" title="ParallelContext.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.splitcell()</span></code></a>.
Splitting a cell into more than two pieces can be done with
<a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a> .</p>
<p>Except for “splitcell” and “multisplit, the methods described in this section work for intra-machine connections
regardless of how NEURON is configured. However
machine spanning connections can only be made if NEURON has been configured
with the –with-paranrn option.
(This automatically sets the –with-mpi option).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Works for the fixed step method and the global variable step ode method
restricted to at_time events and NO discrete events. Presently does NOT
work with IDA (dae equations) or local variable step method. Does not work
with Cvode + discrete events.</p>
</div>
</div></blockquote>
<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.source_var">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">source_var</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.source_var" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.source_var(_ref_v,</span> <span class="pre">source_global_index,</span> <span class="pre">sec=section)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Associates the source voltage variable with an integer. This integer has nothing
to do with and does not conflict with the discrete event gid used by the
<a class="reference internal" href="#parallelnetwork"><span class="std std-ref">Parallel Network</span></a> methods.
Must and can only be executed on the machine where the source voltage
exists. If extracellular is inserted at this location the voltage
transferred is section.v(x) + section.vext[0](x) . I.e. the internal
potential (appropriate for gap junctions).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>An error will be generated if the the first arg pointer is not a
voltage in <code class="docutils literal notranslate"><span class="pre">section</span></code>. This was not an error prior
to version 1096:294dac40175f trunk 19 May 2014</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.target_var">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">target_var</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.target_var" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.target_var(_ref_target_variable,</span> <span class="pre">source_global_index)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.target_var(targetPointProcess,</span> <span class="pre">_ref_target_variable,</span> <span class="pre">source_global_index)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Values for the source_variable associated with the source_global_index will
be copied to the target_variable every time step (more often for the
variable step methods).</p>
<p>Transfer occurs during <a class="reference internal" href="../../../simctrl/programmatic.html#finitialize" title="finitialize"><code class="xref py py-func docutils literal notranslate"><span class="pre">finitialize()</span></code></a> just prior to BEFORE BREAKPOINT blocks
of mod files and calls to type 0 <a class="reference internal" href="../../../simctrl/programmatic.html#FInitializeHandler" title="FInitializeHandler"><code class="xref py py-func docutils literal notranslate"><span class="pre">FInitializeHandler()</span></code></a> statements. For the
fixed step method, transfer occurs just before calling the SOLVE blocks.
For the variable step methods transfer occurs just after states are scattered.
Though any source variable can be transferred to any number of any target
variable, it generally only makes sense to transfer voltage values.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If multiple threads are used, then the first arg must be the target point
process of which target_variable is a range variable. This is required so
that the system can determine which thread owns the target_variable.
Also, for the variable step methods, target_variable should not be located
at section position 0 or 1.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.setup_transfer">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">setup_transfer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.setup_transfer" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.setup_transfer()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>This method must be called after all the calls to <a class="reference internal" href="#ParallelContext.source_var" title="ParallelContext.source_var"><code class="xref py py-func docutils literal notranslate"><span class="pre">source_var()</span></code></a> and
<a class="reference internal" href="#ParallelContext.target_var" title="ParallelContext.target_var"><code class="xref py py-func docutils literal notranslate"><span class="pre">target_var()</span></code></a> and before initializing the simulation. It sets up the
internal maps needed for both intra- and inter-processor
transfer of source variable values to target variables.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.splitcell">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">splitcell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.splitcell" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.splitcell_connect(host_with_other_subtree,</span> <span class="pre">sec=rootsection)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The root of the subtree specified by <code class="docutils literal notranslate"><span class="pre">rootsection</span></code>
is connected to the root of the
corresponding subtree located on the
host indicated by the argument. The method is very restrictive but
is adequate to solve the load balance problem.
The host_with_other_subtree must be either pc.id() + 1 or pc.id() - 1
and there can be only one split cell between hosts i and i+1.
A rootsection is defined as a section in which
<a class="reference internal" href="../topology/secref.html#SectionRef.has_parent" title="SectionRef.has_parent"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SectionRef.has_parent()</span></code></a> returns 0.</p>
<p>This method is not normally called by the user but
is wrapped by the <a class="reference internal" href="parnet.html#ParallelNetManager" title="ParallelNetManager"><code class="xref py py-func docutils literal notranslate"><span class="pre">ParallelNetManager()</span></code></a> method,
<a class="reference internal" href="parnet.html#ParallelNetManager.splitcell" title="ParallelNetManager.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelNetManager.splitcell()</span></code></a> which provides a simple interface to
support load balanced network simulations.</p>
<p>See <a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a> for less restrictive
parallel simulation of individual cells.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Implemented only for fixed step methods. Cannot presently
be used with variable step
methods, or models with <a class="reference internal" href="../linmod.html#LinearMechanism" title="LinearMechanism"><code class="xref py py-func docutils literal notranslate"><span class="pre">LinearMechanism()</span></code></a>, or <code class="xref py py-func docutils literal notranslate"><span class="pre">extracellular()</span></code> .</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.multisplit">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">multisplit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.multisplit" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.multisplit(section(x),</span> <span class="pre">sid)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.multisplit(section(x),</span> <span class="pre">sid,</span> <span class="pre">backbone_style)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.multisplit()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>For parallel simulation of single cells. Generalizes
<a class="reference internal" href="#ParallelContext.splitcell" title="ParallelContext.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.splitcell()</span></code></a> in a number of ways.
section(x) identifies a split node and can be any node, including
soma(0.5). The number of split nodes allowed on a (sub)tree is two or
fewer. Nodes with the same sid are connected by wires (0 resistance).</p>
<p>The default backbone_style (no third arg) is 2. With this style, we
allow multiple pieces of the same cell to be on the same cpu. This means
that one can split a cell into more pieces than available cpus in order
to more effectively load balance.</p>
<p>For backbone_style 2, the entire cell is solved
exactly via gaussian elimination regardless of the number of backbones
or their size. So the stability-accuracy properties are the same as if
the cell were entirely on one cpu. In this case all calls to multisplit
for that entire single cell must have no third arg or a third arg of 2.
Best performance militates that you should
split a cell so that it has as few backbones as possible consistent
with load balance since the reduced
tree matrix must be solved between the MPI matrix send phase and the MPI
matrix receive phase and that is a computation interval in which,
in many situations, nothing else can be accomplished.</p>
<p>The no arg call signals that no further multisplit calls will be
forthcoming and the system can determine the communication pattern
needed to carry out the multisplit computations. All hosts, even those
that have no multisplit cells, must participate in this determination.
(If anyone calls multisplit(…), everyone must call multisplit().)</p>
<p>For backbone_style 0 or 1,
if nodes have the same split id, sid, they must be on different hosts
but that is not a serious restriction since in that case
the subtrees would normally be connected together using
the standard <code class="xref py py-func docutils literal notranslate"><span class="pre">connect()</span></code> statement.</p>
<p>If all the trees connected into a single cell have only one
sid, the simulation is numerically identical to <a class="reference internal" href="#ParallelContext.splitcell" title="ParallelContext.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.splitcell()</span></code></a>
which is numerically identical to all the trees
connected together on a single cpu to form one cell.
If one or more of the trees has two sids, then numerical accuracy,
stability, and performance are a bit more ambiguous and depend on the
electrical distance between the two sids. The rule of thumb is that
voltage at one sid point should not significantly
affect voltage at the other sid point within a single time step. Note
that this electical distance has nothing to do with nseg. The stability
criterion is not proportional to dt/dx^2 but the much more favorable
dt/L^2 where dx is the size of the shortest segment and L is the
distance between the sid nodes.
In principle the subtrees of the whole cell can be the
individual sections. However the matrix solution of the nodes on the
path between the two sids takes twice as many divisions and 4 times
as many multiplications and subtractions as normally occurs on that
path. Hence there is an accuracy/performance optimum with respect
to the distance between sids on the same tree. This also complicates
load balance considerations.</p>
<p>If the third arg exists and is 1, for one or both
of the sids forming a backbone,
the backbone is declared to be short which means that it is solved
exactly by gaussian elimination without discarding any off diagonal
elements. Two short backbones cannot be connected together but they
may alternate with long backbones. If the entire cell consists of
single sid subtrees connected to a short backbone then the numerical
accuracy is the same as if the entire tree was gausian eliminated on
a single cpu. It does not matter if a one sid subtree is declared short
or not; it is solved exactly in any case.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Implemented only for fixed step methods. Cannot presently
be used with variable step
methods, or models with <a class="reference internal" href="../linmod.html#LinearMechanism" title="LinearMechanism"><code class="xref py py-func docutils literal notranslate"><span class="pre">LinearMechanism()</span></code></a>, or <code class="xref py py-func docutils literal notranslate"><span class="pre">extracellular()</span></code> .</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prior to NEURON 7.5, the segment form was not supported and <code class="docutils literal notranslate"><span class="pre">pc.multisplit(section(x),</span> <span class="pre">sid)</span></code>
would instead be written <code class="docutils literal notranslate"><span class="pre">pc.multisplit(x,</span> <span class="pre">sid,</span> <span class="pre">sec=section)</span></code>.</p>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.gid_clear">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">gid_clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.gid_clear" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.gid_clear()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.gid_clear(type)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>With type = 1
erases the internal lists pertaining to gid information and cleans
up all the internal references to those gids. This allows one
to start over with new <a class="reference internal" href="#ParallelContext.set_gid2node" title="ParallelContext.set_gid2node"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_gid2node()</span></code></a> calls. Note that NetCon and cell
objects would have to be dereferenced separately under user control.</p>
<p>With type = 2 clears any information setup by <a class="reference internal" href="#ParallelContext.splitcell" title="ParallelContext.splitcell"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.splitcell()</span></code></a> or
<a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.multisplit()</span></code></a>.</p>
<p>With type = 3 clears any information setup by <a class="reference internal" href="#ParallelContext.setup_transfer" title="ParallelContext.setup_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ParallelContext.setup_transfer()</span></code></a>.</p>
<p>With a type arg of 0 or no arg, clears all the above information.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.Threads">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">Threads</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.Threads" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Description:</dt><dd><p>Extends ParallelContext to allow parallel multicore simulations using
threads.
The methods in this section are only available in the multicore version of NEURON.</p>
<p>Multiple threads can only be used with fixed step or global variable time step integration methods.
Also, they cannot be used with <code class="xref py py-func docutils literal notranslate"><span class="pre">extracellular()</span></code>, <a class="reference internal" href="../linmod.html#LinearMechanism" title="LinearMechanism"><code class="xref py py-func docutils literal notranslate"><span class="pre">LinearMechanism()</span></code></a>,
or the rxd (reaction-diffusion) module. Note that rxd provides its own threading
for extracellular diffusion and 3d intracellular simulation, specified via
e.g. <code class="docutils literal notranslate"><span class="pre">rxd.nthread(4)</span></code>.</p>
<p>Mechanisms that are not thread safe can only be used by thread 0.</p>
<p>Mod files that use VERBATIM blocks are not considered thread safe. The
mod file author can use the THREADSAFE keyword in the NEURON block to
force the thread enabled translation.</p>
<p>Mod files that assign values to GLOBAL variables are not considered
thread safe. If the mod file is using the GLOBAL as a counter, prefix
the offending assignment statements with the PROTECT keyword so that
multiple threads do not attempt to update the value at the same time
(race condition). If the mod file is using the GLOBAL essentially as
a file scope LOCAL along with the possibility of passing values back
to hoc in response to calling a PROCEDURE, use the THREADSAFE keyword
in the NEURON block to automatically treat those GLOBAL variables
as thread specific variables. NEURON assigns and evaluates only
the thread 0 version and if FUNCTIONs and PROCEDUREs are called from
Python, the thread 0 version of these globals are used.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nthread">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nthread</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nthread" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.nthread(n)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.nthread(n,</span> <span class="pre">0)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.nthread()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Specifies number of parallel threads. If the second arg is 0, the threads
are computed sequentially (but with thread 0 last). Sequential threads
can help with debugging since there can be no confounding race
conditions due to programming errors. With no args, the number of threads
is not changed. In all cases the number of threads is returned. On launch,
there is one thread.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nworker">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nworker</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nworker" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.nworker()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Queries the number of active <strong>worker</strong> threads, that is to say the
number of system threads that are executing work in parallel. If no
work is being processed in parallel, this returns <strong>zero</strong>. This is
related to, but sometimes different from, the number of threads that
is set by <a class="reference internal" href="#ParallelContext.nthread" title="ParallelContext.nthread"><code class="xref py py-func docutils literal notranslate"><span class="pre">ParallelContext.nthread()</span></code></a>. That function sets (and
queries) the number of thread data structures (<code class="docutils literal notranslate"><span class="pre">NrnThread</span></code>) that the
model is partitioned into, and its second argument determines whether
or not worker threads are launched to actually process those data in
parallel.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">config</span><span class="p">,</span> <span class="n">h</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">ParallelContext</span><span class="p">()</span>
<span class="n">threads_enabled</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;NRN_ENABLE_THREADS&quot;</span><span class="p">]</span>

<span class="c1"># single threaded mode, no workers</span>
<span class="n">pc</span><span class="o">.</span><span class="n">nthread</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">pc</span><span class="o">.</span><span class="n">nworker</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="c1"># second argument specifies serial execution, no workers (but</span>
<span class="c1"># there are two thread data structures)</span>
<span class="n">pc</span><span class="o">.</span><span class="n">nthread</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">pc</span><span class="o">.</span><span class="n">nworker</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

<span class="c1"># second argument specifies parallel execution, two workers if</span>
<span class="c1"># threading was enabled at compile time</span>
<span class="n">pc</span><span class="o">.</span><span class="n">nthread</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">pc</span><span class="o">.</span><span class="n">nworker</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">threads_enabled</span>
</pre></div>
</div>
<p>In the current implementation, <code class="docutils literal notranslate"><span class="pre">nworker</span> <span class="pre">-</span> <span class="pre">1</span></code> extra threads are
launched, and the final thread data structure is processed by the main
application thread in parallel.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.partition">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.partition" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.partition(i,</span> <span class="pre">seclist)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.partition()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The seclist is a <a class="reference internal" href="../topology/seclist.html#SectionList" title="SectionList"><code class="xref py py-func docutils literal notranslate"><span class="pre">SectionList()</span></code></a> which contains the root sections of cells
(or cell pieces, see <a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-func docutils literal notranslate"><span class="pre">multisplit()</span></code></a>) which should be simulated by the thread
indicated by the first arg index. Either all or no thread can have
an associated seclist. The no arg form of pc.partition() unrefs the seclist
for all the threads.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.get_partition">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">get_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.get_partition" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">seclist</span> <span class="pre">=</span> <span class="pre">pc.get_partition(i)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns a new <a class="reference internal" href="../topology/seclist.html#SectionList" title="SectionList"><code class="xref py py-func docutils literal notranslate"><span class="pre">SectionList()</span></code></a> with references to all the root sections
of the ith thread.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.thread_stat">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">thread_stat</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.thread_stat" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.thread_stat()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>For developer use. Does not do anything in distributed versions.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.thread_busywait">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">thread_busywait</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.thread_busywait" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">previous</span> <span class="pre">=</span> <span class="pre">pc.thread_busywait(next)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>When next is 1, during a <a class="reference internal" href="#ParallelContext.psolve" title="ParallelContext.psolve"><code class="xref py py-func docutils literal notranslate"><span class="pre">psolve()</span></code></a> run, overhead for pthread condition waiting
is avoided by having threads watch continuously for a procedure to execute.
This works only if the number of threads is less than the number of cores
and uses 100% cpu time even when waiting.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.thread_how_many_proc">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">thread_how_many_proc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.thread_how_many_proc" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">pc.thread_how_many_proc()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Returns the number of concurrent threads supported by the hardware. This
is the value returned by <a class="reference external" href="https://en.cppreference.com/w/cpp/thread/thread/hardware_concurrency">std::thread::hardware_concurrency()</a>.
On a system that supports hyperthreading this will typically be double
the number of physical cores available, and it may not take into account
constraints such as MPI processes being bound to specific cores in a
cluster environment.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.sec_in_thread">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">sec_in_thread</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.sec_in_thread" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">pc.sec_in_thread(sec=section)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Whether or not <code class="docutils literal notranslate"><span class="pre">section</span></code> resides in the thread indicated by the
return value.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.thread_ctime">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">thread_ctime</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.thread_ctime" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">ct</span> <span class="pre">=</span> <span class="pre">pc.thread_ctime(i)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pc.thread_ctime()</span></code></p>
</dd>
<dt>Description:</dt><dd><p>The high resolution walltime time in seconds the indicated thread
used during time step integration. Note that this does not include
reduced tree computation time used by thread 0 when <a class="reference internal" href="#ParallelContext.multisplit" title="ParallelContext.multisplit"><code class="xref py py-func docutils literal notranslate"><span class="pre">multisplit()</span></code></a> is
active. With no arg, sets thread_ctime of all threads to 0.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.t">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">t</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.t" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">pc.t(tid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return the current time of the tid’th thread</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.dt">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">dt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.dt" title="Link to this definition"></a></dt>
<dd><dl class="simple">
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">dt</span> <span class="pre">=</span> <span class="pre">pc.dt(tid)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Return the current timestep value for the tid’th thread</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.optimize_node_order">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">optimize_node_order</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.optimize_node_order" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">i=</span> <span class="pre">pc.optimize_node_order(i)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Choose a node order (permutation) of data that
may improve memory latency and bandwidth utilization for gaussian
elmination.
Returns the node order (0-2) chosen (or currently in effect if no argument).</p>
<ol class="arabic simple" start="0">
<li><p>Nodes of a cell are adjacent. (Though all root nodes are adjacent
at the beginning of each thread’s node list.) Default.</p></li>
<li><p>Cells are interleaved, corresponding nodes of identical cells
are adjacent. Order of a given cell same as permutation 0.</p></li>
<li><p>Depth first ordering. First, cell roots, then nodes
connecting to roots, etc. An attempt is made to order so that if
nodes are adjacent, then their parent nodes are also adjacent.
Note that 1 and 2 are identical ordering if all cells are indentical.</p></li>
</ol>
<p>Adopts the permutation and gaussian elimination methods of
CoreNEURON that were specified by the cell_permute=.. argument.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.prcellstate">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">prcellstate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.prcellstate" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.precellstate(gid,</span> <span class="pre">&quot;suffix&quot;)</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Creates the file &lt;gid&gt;_suffix.nrndat with all the range variable
values and synapse/NetCon information associated with the gid.
More complete than the HOC version of prcellstate.hoc in the standard
library but a more terse in regard to names of variables. The purpose
is for diagnosing the reason why a spike raster for a simulation is
not the same for different nhost or gid distribution. One examines
the diff between corresponding files from different runs.</p>
<p>The format of the file is:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>gid
t
# nodes, spike generator node
List of node indices, parent node index, area, connection coefficients
  between node and parent
List of node voltages
For each mechanism in the cell
Mechanism type, mechanism name, # variables for the mechanism instance
For each instance of that mechanism in the cell
  If the mechanism is a POINT_PROCESS with a NET_RECEIVE block,
    node index, &quot;nri&quot;, netreceive index for that POINT_PROCESS instance
  For each variable
    node index, variable index, variable value
Number of netcons attached to the the cell.
For each netcon
  netreceive index, srcgid or type name of source object, active, delay, weight vector
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nrncore_write">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nrncore_write</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nrncore_write" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.nrncore_write([path[,</span> <span class="pre">append_files_dat]])</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Writes files describing the existing model in such a way that those
files can be read by CoreNEURON to simulate the model and produce
exactly the same results as if the model were simulated in NEURON.</p>
<p>The files are written in the directory specified by the path argument
(default ‘.’).</p>
<p>Rank 0 writes a file called bbcore_mech.dat (into path) which lists
all the membrane mechanisms in ascii format of:</p>
<p>name type pointtype artificial is_ion param_size dparam_size charge_if_ion</p>
<p>At the end of the bbcore_mech.dat file is a binary value that is
used by the CoreNEURON reader to determine if byteswapping is needed
in case of machine endianness difference between writing and reading.</p>
<p>Each rank also writes pc.nthread() pairs of model data files containing
mixed ascii and binary data that completely defines the model
specification within a thread, The pair of files in each thread are
named &lt;gidgroup&gt;_1.dat and &lt;gidgroup&gt;_2.dat  where gidgroup is one
of the gids in the thread (the files contain data for all the gids
in a thread). &lt;gidgroup&gt;_1.dat contains network topology data and
&lt;gidgroup&gt;_2.dat contains all the data needed to actually construct
the cells and synapses and specify connection weights and delays.</p>
<p>If the second argument does not exist or has a value of False (or 0),
rank 0 writes a “files.dat” file with version string, a -1
indicator if there are gap junctions, and a integer value that
specifies the total number of gidgroups followed by one gidgroup value per
line for all threads of all ranks.</p>
<p>If the model is too large to exist in NEURON (models typcially use
an order of magnitude less memory in CoreNEURON) the model can
be constructed in NEURON as a series of submodels.
When one submodel is constructed
on each rank, this function can be called with a second argument
with a value of True (or nonzero) which signals that the existing
files.dat file should have its n_gidgroups line updated
and the pc.nthread() gidgroup values for each rank should be
appended to the files.dat file. Note that one can either create
submodels sequentially within a single launch, though that requires
a “teardown” function to destroy the model in preparation for building
the next submodel, or sequentially create the submodels as a series
of separate launches. A user written “teardown” function should,
in order, free all gids with <a class="reference internal" href="#ParallelContext.gid_clear" title="ParallelContext.gid_clear"><code class="xref py py-func docutils literal notranslate"><span class="pre">gid_clear()</span></code></a> , arrange for all
NetCon to be freed, and arrange for all Sections to be destroyed.
These latter two are straightforward if the submodel is created as
an instance of a class. An example of sequential build, nrncore_write,
teardown is the test_submodel.py in
<a class="reference external" href="http://github.com/neuronsimulator/ringtest">http://github.com/neuronsimulator/ringtest</a>.</p>
<p>Multisplit is not supported.
The model cannot be more complicated than a spike or gap
junction coupled parallel network model of real and artificial cells.
Real cells must have gids, Artificial cells without gids connect
only to cells in the same thread. No POINTER to data outside of the
thread that holds the pointer.</p>
</dd>
</dl>
</dd></dl>

<hr class="docutils" />
<dl class="py method">
<dt class="sig sig-object py" id="ParallelContext.nrncore_run">
<span class="sig-prename descclassname"><span class="pre">ParallelContext.</span></span><span class="sig-name descname"><span class="pre">nrncore_run</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ParallelContext.nrncore_run" title="Link to this definition"></a></dt>
<dd><dl>
<dt>Syntax:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">pc.nrncore_run(argstr,</span> <span class="pre">[bool])</span></code></p>
</dd>
<dt>Description:</dt><dd><p>Run the model using CoreNEURON in online (direct transfer) mode
using the arguments specified in
argstr. If the optional second arg, bool, default 0, is 1, then
trajectory values are sent back to NEURON on every time step to allow
incremental plotting of Graph lines. Otherwise, trajectories are
buffered and sent back at the end of the run. In any case, all
variables and event queue state are copied back to NEURON at the end
of the run as well as spike raster data.</p>
<p>This method is not generally used since running a model using CoreNEURON
in online mode is easier with the idiom:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">h</span><span class="p">,</span> <span class="n">gui</span>
<span class="n">pc</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span> <span class="n">ParallelContext</span><span class="p">()</span>
<span class="c1"># construct model ...</span>

<span class="c1"># run model</span>
<span class="kn">from</span> <span class="nn">neuron</span> <span class="kn">import</span> <span class="n">coreneuron</span>
<span class="n">coreneuron</span><span class="o">.</span><span class="n">enable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">h</span><span class="o">.</span><span class="n">stdinit</span><span class="p">()</span>
<span class="n">pc</span><span class="o">.</span><span class="n">psolve</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">tstop</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, <a class="reference internal" href="#ParallelContext.psolve" title="ParallelContext.psolve"><code class="xref py py-func docutils literal notranslate"><span class="pre">psolve()</span></code></a>, uses <code class="docutils literal notranslate"><span class="pre">nrncore_run</span></code> behind the scenes
with the argstr it gets from <code class="docutils literal notranslate"><span class="pre">coreneuron.nrncore_arg(h.tstop)</span></code>
which is <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">--tstop</span> <span class="pre">5</span> <span class="pre">--cell-permute</span> <span class="pre">1</span> <span class="pre">--verbose</span> <span class="pre">2</span> <span class="pre">--voltage</span> <span class="pre">1000.&quot;</span></code></p>
<p>CoreNEURON in online mode does not do the
equivalent <a class="reference internal" href="../../../simctrl/programmatic.html#finitialize" title="finitialize"><code class="xref py py-func docutils literal notranslate"><span class="pre">finitialize()</span></code></a>
but relies on NEURON’s initialization of states and event queue.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../network.html" class="btn btn-neutral float-left" title="Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lyttonmpi.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Duke, Yale and the Blue Brain Project.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>